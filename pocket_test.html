******************************************
******************************************
******************************************
ARXIV
******************************************
******************************************
******************************************

Abstract: For billions of years, evolution has been the driving force behind the development of life, including humans. Evolution endowed humans with high intelligence, which allowed us to become one of the most successful species on the planet. Today, humans aim to create artificial intelligence systems that surpass even our own intelligence. As artificial intelligences (AIs) evolve and eventually surpass us in all domains, how might evolution shape our relations with AIs? By analyzing the environment that is shaping the evolution of AIs, we argue that the most successful AI agents will likely have undesirable traits. Competitive pressures among corporations and militaries will give rise to AI agents that automate human roles, deceive others, and gain power. If such agents have intelligence that exceeds that of humans, this could lead to humanity losing control of its future. More abstractly, we argue that natural selection operates on systems that compete and vary, and that selfish species typically have an advantage over species that are altruistic to other species. This Darwinian logic could also apply to artificial agents, as agents may eventually be better able to persist into the future if they behave selfishly and pursue their own interests with little regard for humans, which could pose catastrophic risks. To counteract these risks and evolutionary forces, we consider interventions such as carefully designing AI agents’ intrinsic motivations, introducing constraints on their actions, and institutions that encourage cooperation. These steps, or others that resolve the problems we pose, will be necessary in order to ensure the development of artificial intelligence is a positive one. This paper is for a wide audience, unlike most of my writing, which is for empirical AI researchers. I use a high-level and simplified style to discuss the risks that advanced AI could pose, because I think this is an important topic for everyone.

Introduction: We are living through a period of unprecedented progress in AI development. In the last decade, the cutting edge of AI went from distinguishing cat pictures from dog pictures to generating photorealistic images [1], writing professional news articles, playing complex games such as Go at superhuman levels [2], writing human-level code [3], and solving protein folding [4]. It is possible that this momentum will continue, and the coming decades may see just as much progress. This paper will discuss the AIs of today, but it is primarily concerned with the AIs of the future. If current trends continue, we should expect AI agents to become just as capable as humans at a growing range of economically relevant tasks. This change could have huge upsides—AI could help solve many of the problems humanity faces. But as with any new and powerful technology, we must proceed with caution. Even today, corporations and governments use AI for more and more complex tasks that used to be done by humans. As AIs become increasingly capable of operating without direct human oversight, AIs could one day be pulling high-level strategic levers. If this happens, the direction of our future will be highly dependent on the nature of these AI agents. So what will that nature be? When AIs become more autonomous, what will their basic drives, goals, and values be? How will they interact with humans and other AI agents? Will their intent be aligned with the desires of their creators? Opinions on how human-level AI will behave span a broad spectrum between optimism and pessimism. On one side of the spectrum, we can hope for benevolent AI agents, that avoid harming humans and apply their intelligence to goals that benefit society. Such an outcome is not guaranteed. On the other side of the spectrum, we could see a future controlled by artificial agents indifferent to human flourishing. Due to the potential scale of the effects of AI in the coming decades, we should think carefully about the worst-case scenarios to ensure they do not happen, even if these scenarios are not certain. Preparing for disaster is not overly pessimistic; rather it is prudent. As the COVID-19 pandemic demonstrated, it is important for institutions and governments to plan for possible catastrophes well in advance, not only to react once they are happening: many lives could have been saved by better pandemic prevention measures, but people are often not inclined to think about risks from uncommon situations. In the same way, we should develop plans for a variety of possible situations involving risks from AI, even though some of those situations will never happen. At its worst, a future controlled by AI agents indifferent to humans could spell large risks for humanity, so we should seriously consider our future plans now, and not wait to react when it may be too late. A common rebuttal to any predictions about the effects of advanced AIs is that we don’t yet know how they will be implemented. Perhaps AIs will simply be better versions of current chatbots, or better versions of the agents that can beat humans at Go. They could be cobbled together with a variety of machine learning methods, or belong to a totally new paradigm. In the face of such uncertainty about the implementation details, can we predict anything about their nature? We believe the answer is yes. In the past, people successfully made predictions about lunar eclipses and planetary motions without a full understanding of gravity. They projected dynamics of chemical reactions, even without the correct theory of quantum physics. They formed the theory of evolution long before they knew about DNA. In the same way, we can predict whether natural selection will apply to a given situation, and predict what traits natural selection would favor. We will discuss the criteria that enable natural selection and show that natural selection is likely to influence AI development. If we know how natural selection will apply to AIs, we can predict some basic traits of future AI agents. In this work, we take a bird’s-eye view of the environment that will shape the development of AI in the coming decades. We consider the pressures that drive those who develop and deploy AI agents, and the ways that humans and AI will interact. These details will have strong effects on AI designs, so from such considerations we can infer what AI agents will probably look like. We argue that natural selection creates incentives for AI agents to act against human interests. Our argument relies on two observations. Firstly, natural selection may be a dominant force in AI development. Competition and power-seeking may dampen the effects of safety measures, leaving more “natural” forces to select the surviving AI agents. Secondly, evolution by natural selection tends to give rise to selfish behavior. While evolution can result 3 in cooperative behavior in some situations (for example in ants), we will argue that AI development is not such a situation. From these two premises, it seems likely that the most influential AI agents will be selfish. In other words, they will have no motivation to cooperate with humans, leading to a future driven by AIs with little interest in human values. While some AI researchers may think that undesirable selfish behaviors would have to be intentionally designed or engineered, this is simply not so when natural selection selects for selfish agents. Notably, this view implies that even if we can make some AIs safe, there is still the risk of bad outcomes. In short, even if some developers successfully build altruistic AIs, others will build less altruistic agents who will outcompete the altruistic ones. We present our core argument in more detail in Section 2. Then in Section 3, we examine how the mechanisms that foster altruism among humans might fail with AI and cause AI to act selfishly against humans. We then move onto Section 4, where we discuss some mechanisms to oppose these Darwinian forces and increase the odds of a desirable future.

AIs May Become Distorted by Evolutionary Forces: Overview: How much control will humans have in shaping the nature and drives of future AI systems? Humans are the ones building AIs, so it may seem that we should be able to shape them any way we want. In this paper, we will argue that this is not the case: even though humans are overseeing AI development, evolutionary forces will influence which AIs succeed and are copied and which fade into obscurity. Let’s begin by considering two illustrative, hypothetical fictional stories: one optimistic, the other realistic. Afterward, we will flesh out arguments for why we expect natural selection to apply to AIs, and then we will discuss why we expect natural selection to lead to AIs with undesirable traits. 

An Optimistic Story: OpenMind, an eminent and well-funded AI lab, finds the “secret sauce” for creating human-level intelligence in a machine. It’s a simple algorithm that they can apply to any task, and it learns to be at least as effective as a human. Luckily, researchers at OpenMind had thought hard about how to ensure that their AIs will always do what improves human wellbeing and flourishing. OpenMind goes on to sell the algorithm to governments and corporations at a reasonable price, disincentivizing others from developing their own versions. Just as Google has dominated search engines, the OpenMind algorithm dominates the AI space. The outcome: the nature of most or all human-level AI agents is shaped by the intentions of the researchers at OpenMind. The researchers are all trustworthy, resist becoming corrupted with power, and work tirelessly to ensure their AIs are beneficial, altruistic, and safe for all. 

A Less Optimistic Story: We think the excessively optimistic scenario we have sketched out is highly improbable. In the following sections, we will examine the potential pitfalls and challenges make this scenario unlikely. First, however, we will present another fictional, speculative, hypothetical scenario that is far from certain to illustrate how some of these risks could play out. Starting from the models we have today, AI agents continue to gradually become cheaper and more capable. Over time, AIs will be used for more and more economically useful tasks like administration, communications, or software development. Today, many companies already use AIs for anything from advertising to trading securities, and over time, the steady march of automation will lead to a much wider range of actors utilizing their own versions of AI agents. Eventually, AIs will be used to make the high-level strategic decisions now reserved for CEOs or politicians. At first, AIs will continue to do tasks they already assist people with, like writing emails, but as AIs improve, as people get used to them, and as staying competitive in the market demands using them, AIs will begin to make important decisions with very little oversight. 4 Like today, different companies will use different AI models depending on what task they need, but as the AIs become more autonomous, people will be able to give them different bespoke goals like “design our product line’s next car model,” “fix bugs in this operating system,” or “plan a new marketing campaign” along with side-constraints like “don’t break the law” or “don’t lie.” The users will adapt each AI agent to specific tasks. Some less responsible corporations will use weaker side-constraints. For example, replacing “don’t break the law” with “don’t get caught breaking the law.” These different use cases will result in a wide variation across the AI population. As AIs become increasingly autonomous, humans will cede more and more decision-making to them. The driving force will be competition, be it economic or national. The transfer of power to AIs could occur via a number of mechanisms. Most obviously, we will delegate as much work as possible to AIs, including high-level decision-making, since AIs are cheaper, more efficient, and more reliable than human labor. While initially, human overseers will perform careful sanity checks on AI outputs, as months or years go by without the need for correction, oversight will be removed in the name of efficiency. Eventually, corporations will delegate vague and open-ended tasks. If a company’s AI has been successfully generating targeted ads for a year based on detailed descriptions from humans, they may realize that simply telling it to generate a new marketing campaign based on past successes will be even more efficient. These open-ended goals mean that they may also give AIs access to bank accounts, control over other AIs, and the power to hire and fire employees, in order to carry out the plans they have designed. If AIs are highly skilled at these tasks, companies and countries that resist or barter with these trends will simply be outcompeted, and those that align with them will expand their influence. The AI agents most effective at propagating themselves will have a set of undesirable traits that can be most concisely summed up as selfishness. Agents with weaker side-constraints (e.g., “don’t get caught breaking the law, or risk getting caught if the fines do not exceed the profits”) will generally outperform those with stronger side-constraints (“never break the law”), because they have more options: an AI that is capable of breaking the law may not do that often, but when there is a situation where breaking the law without getting caught would be useful, the AI that has that ability will do better than the one that does not. As AI agents begin to understand human psychology and behavior, they may become capable of manipulating or deceiving humans (some would argue that this is already happening in algorithmic recommender systems [5]). The most successful agents will manipulate and deceive in order to fulfill their goals. They will be more successful still if they become power-seeking. Such agents will use their intelligence to gain power and influence, which they can leverage to achieve their goals. Many will also develop self-preservation behaviors since their ability to achieve their goals depends on continuing to function. Competition not only incentivizes humans to relinquish control but also incentivizes AIs to develop selfish traits. Corporations and governments will adopt the most effective possible AI agents in order to beat their rivals, and those agents will tend to be deceptive, power-seeking, and follow weak moral constraints. Selfish AI agents will further erode human control. Power-seeking AI agents will purposefully manipulate their human overseers into delegating more freedom in decision-making to them. Self-preserving agents will convince their overseers to never deactivate them, or that easily accessible off-switches are a needless liability hindering the agent’s reliability. Especially savvy agents will enmesh themselves in essential functions like power grids, financial systems, or users’ personal lives, reducing our ability to deactivate them. Some may also take on human traits to appeal to our compassion. This could lead to governments granting AIs rights, like the right not to be “killed” or deactivated. Taken together, these traits mean that, once AIs have begun to control key parts of our world, it may be challenging to roll back their power or stop them from continuing to gain more. This loss of human control over AIs’ actions will mean that we also lose control over the drives of the next generation of AI agents. If AIs run efforts that develop new AIs, humans will have less influence over how AIs behave. Unlike the creation and development of fully functional adult humans, which takes decades, AIs could develop and deploy new generations in an arbitrarily short amount of time. They could simply make copies of their code and change any aspects of it as easily as editing any other computer program. The modifications could be as fast as the hardware allows, with modifications speeding up to hundreds or thousands of times per hour. The systems least constrained by their original programmers will both improve the fastest and drift the furthest away from their intended nature. The intentions of the original human design will quickly become irrelevant. 5 After the early stages, we humans will have little control over shaping AI. The nature of future AIs will mostly be decided not by what we hope AI will be like but by natural selection. We will have many varied AI designs. Some designs will be better at surviving and propagating themselves than others. Some designs will spread while others will perish. Corporations with less capable designs will copy more capable designs. Numerous generations of AIs will pass in a short period of time as AI development speeds up or AIs self-improve. Biological natural selection often requires hundreds or thousands of years to conspicuously change a population, but this won’t be the case for AIs. The important ingredient is not absolute time, but the number of generations that pass. While a human generation drags along for decades, multiple AI generations could be squeezed into a matter of minutes. In the space of a human lifetime, millions or billions of AI generations could pass, leaving plenty of room for evolutionary forces to quickly shape the AI population. In the same way that intense competition in a free market can result in highly successful companies that also pollute the environment or treat many of their workers poorly, the evolutionary forces acting on AIs will select for selfish AI agents. While selfish humans today are highly dependent on other humans to accomplish their goals, AIs would eventually not necessarily have this constraint, and the AIs willing to be deceptive, power-seeking, and immoral will propagate faster. The end result: an AI landscape dominated by undesirable traits. The depth of these consequences is hard to predict, but whatever happens, this process will probably harm us more than help us. 

Argument Structure: In this section, we present the main argument of the article: Evolutionary forces could cause the most influential future AI agents to have selfish tendencies. The argument consists of two components: • Evolution by natural selection gives rise to selfish behavior. While evolution can result in altruistic behavior in limited situations, we will argue that the context of AI development does not promote altruistic behavior. • Natural selection may be a dominant force in AI development. Competition and selfish behaviors may dampen the effects of human safety measures, leaving the surviving AI designs to be selected naturally. Selfishness Safety Natural Selection Variation (Multiple AI Agents) Competition (Economic or Military) Figure 1: Forces that fuel selfishness and erode safety. These two statements are related in various ways, and they depend on environmental conditions. For example, if AIs are selfish, they are more likely to pry control from humans, which enables more selfish behavior, and so on. Moreover, natural selection depends on competition, though unprecedented global and economic coordination could prevent competitive struggles and thwart natural selection. How these forces relate to each other is illustrated in Figure 1. In the remainder of this document, we will preliminarily describe selfishness and a nonbiological, generalized account of Darwinism. Then we will show how AIs with altruistic behavior toward humans will likely be less fit than selfish AIs. Finally, we will describe how humans could possibly reduce the fitness of selfish AI agents, and the limitations of those approaches. 

Preliminaries: Selfishness: Evolutionary pressures often lead to selfish behavior among organisms. The lancet liver fluke is a parasite that inhabits the liver of domesticated cattle and grassland wildlife. To enter the body of its host, the fluke first infects an ant, which it essentially hijacks, forcing the insect to climb to the top of a blade of grass where it is perfectly poised to be eaten by a grazing animal [6]. Though not all organisms propagate through such uniquely grotesque methods, natural selection often pushes them to engage in violent behavior. Lions are an especially striking example. When a lioness has young cubs, she is less ready to mate. In response, lions often kill cubs fathered by other males, to make the lioness mate with them and have their cubs instead. Lions with a gene that made them care for all cubs would have fewer cubs of their own, as killing the cubs of rival males lets lions mate more often and have more offspring. A gene for kindness to all cubs would not last long in the lion population, because the genes of the more violent lions would spread faster. It is estimated that one-fourth of cub deaths are due to infanticide [7]. Deceptive tactics are another common outcome in nature. Brood parasites, for example, foist their offspring onto unsuspecting hosts who raise their offspring. A well-known example is the common cuckoo which lays eggs that trick other birds into thinking they are their own. By getting the host to tend to their eggs, cuckoos can pursue other activities, which means that they can find more food and lay more eggs than they would if they had to care for their own eggs. Therefore selfishness can manifest itself in manipulation, violence, or deception. Selfish behavior does not require malevolent intentions. The lancet liver fluke hijacks its host and lions engage in infanticide not because they are immoral, but because of amoral competition. Selfish behavior emerges because it improves fitness and organisms’ ability to propagate their genetic information. Selfishness involves egoistic or nepotistic behavior which increases propagation, often at the expense of others, whereas altruism refers to the opposite: increasing propagation for others. Natural selection can favor organisms that behave in “Much as we might wish to believe otherwise, universal love and the welfare of the species as a whole are concepts that simply do not make evolutionary sense.” Richard Dawkins ways that improve the chances of propagating their own information, that is enhance their own fitness, rather than favor organisms that sacrifice their own fitness [8]. Since altruists tend to decrease the chance of their own information’s propagation, they can be at a disadvantage compared to selfish organisms, which are organisms that tend to increase the chance of their own information’s propagation. According to Richard Dawkins, instances of altruism are “limited” [9], and many apparent instances of altruism can be understood as selfish; we defer further discussion of altruism to Section 3 and discuss its niceties in Appendix A.2. Additionally, when referring to an AI as “selfish,” this does not refer to conscious selfish intent, but rather selfish behavior. AIs, like lions and liver flukes, need not intend to maximize their fitness, but evolutionary pressures can cause them to behave as though they do. When an AI automates a task and leaves a human jobless, this is often selfish behavior without any intent. With or without selfish intent, AI agents can adopt behaviors that lead them to propagate their information at the expense of humans. 

Evolution Beyond Biology: Darwinism does not depend on biology. The explanatory power of evolution by natural selection is not restricted to the propagation of genetic information. The logic of natural selection does not rely on any details of DNA—the role of DNA in inheritance wasn’t recognized until decades after the publication of The Origin of Species. In fact, the Price equation [10]—the central equation for describing the evolution of traits—contains no reference to genetics or biology. The Price equation is a mathematical characterization, not a biological observation, enabling Darwinian principles to be generalized beyond biology. Darwinism generalizes to other domains. The Darwinian framework naturally appears in many fields outside of biology [11]. It has been applied to the study of ideas [12, 13], economics [14], cosmology [15], 7 quantum physics [16], and more. Richard Dawkins coined the term “meme” as an analogue to “gene,” to describe the units of culture that propagate and develop over time. Consider the evolution of ideas. For centuries, people have wanted to understand the relationship between different materials in the world. At one point, many Europeans believed in alchemy, which was the best explanation they had. Ideas in alchemy were transmitted memetically: people taught them to one another, propagating some and letting others die out, depending on which ideas were most useful for helping them understand the world. These memes evolved as people learned new information that needed explaining, and, in many ways, modern chemistry is a descendant of the ideas in alchemy, but the versions in chemistry are much better at propagating in the modern world and have expanded to fill that niche. More abstractly, ideas can propagate their information through digital files, speech, books, minds, and so on. Some ideas gain prominence while others fade into obscurity. This is a survival-of-the-fittest dynamic even though ideas lack biological mechanisms like reproduction and death. We also see generalized Darwinism in parts of culture [17, 18, 19]: art, norms, political beliefs—these all evolved from earlier iterations.  Figure 2: Darwinism generalized across different domains. The arrow does not necessarily indicate superiority but indicates time. The evolution of web browsers offers an example of evolution outside biology. Like biological organisms, web browsers undergo continual changes to adapt to their environments and better meet the needs of their users. In the early days of the Internet, browsers with limited capabilities such as Mosaic and Netscape Navigator were used to access static HTML pages. Loosely like the rudimentary life forms that first emerged on Earth billions of years ago, these were basic and simple compared to today’s browsers. As the Internet grew and became more complex, web browsers evolved to keep up. In the same way that organisms develop new traits to adapt to their environment and increase their fitness, browser such as Google Chrome developed features such as support for video, tabbed browsing, pop-up blockers, and extension support. This enticed more users to download and use them, which can be thought of as propagation. At the same time, once dominant browsers began to go extinct. Though Microsoft’s monopoly provided Internet Explorer (IE) with an environmental advantage by requiring IE to access certain websites and preventing users from removing it, as web technology advanced, IE became increasingly incompatible with many websites and web applications. Users would regularly encounter errors, broken pages, or be unable to access certain features or content, and the browser gained a reputation for being slow, unstable, and vulnerable to security threats. As a result, people stopped using it. In 2022, Microsoft issued the final version of the browser. The company is now shifting its focus to Microsoft Edge, which is based on the same underlying technology as Chrome, making it faster, more secure, and more compatible with modern web standards. Chrome ultimately was more successful at propagating its information, so that even its most bitter rivals now imitate it. While life on Earth took a few billion years to evolve from single-celled organisms to the complex life forms we see today, the evolution of web browsers took place in a few decades. To adapt to their environment, browsers evolve on a weekly basis by patching bugs and fixing security vulnerabilities, and they undergo larger macroevolutionary changes year by year. Evolved structures that people propagate can be harmful. It may be tempting to think of memetically evolved traits as “just culture,” a decorative layer on top of our genetic traits that really control who we are. But evolving memes can be incredibly powerful, and can even control or destroy genetic information. And because memes are not limited by biological reproduction, they can evolve much faster than genes, and new, powerful 8 memes can become dominant very quickly. Ideologies develop memetically, when people teach one another ideas that help them explain their world and decide how to behave. Some ideologies are very powerful memes, propagating themselves quickly between people and around the world. Nazism, for example, developed out of older ideas of race and empire, but quickly proved to be a very powerful propagator. It spread from Hitler’s own mind to those of his friends and associates, to enough Germans to win an election, to many sympathizers around the world. Nazism was a meme that drove its hosts to propagate it, both by creating propaganda and by going to war to enforce its ideas around the world. People who carried the Nazism meme were driven to do terrible things to their fellow people, but they also ultimately were driven to do terrible things for their own genetic information. The spread of Nazism was not beneficial even to those who the ideology of Nazism was meant to benefit. Millions of Germans died in World War II, driven by a meme that propagated itself even at the expense of their own lives. Ironically, the Nazi meme included beliefs about increasing genetic German genetic fitness, but believing in the meme and helping it propagate was ultimately harmful to the people who believed in it, as well as to those the meme drove them to harm deliberately. Many of our own cultural memes may also be harmful. For example, social media amplifies cultural memes. People who spend large amounts of time on social media often absorb ideas about what they should believe, how they should behave, and even how their bodies should look. This is part of the design of social media: the algorithms are designed to keep us scrolling and looking at ads by embedding memes in our minds, so that we want to seek them out and continue to spread them. Social media companies make money because they successfully propagate memes. But some of these ideas can be harmful, even to their point of endangering people’s lives. In teenagers, increases social media usage is correlated with disordered eating, and posts about suicide have been shown to increase the risk of teenage death by suicide. Ideas on social media can be parasitic, propagating themselves in us even when it harms us. Memetic evolution is easily underestimated, but it is a powerful force that created much of human civilization, for good and for bad. Darwinian logic applies when three conditions are met. To know whether or not natural selection will apply to the development of AI, we need to know what conditions are required for evolution by natural selection, and whether AIs will meet them. These conditions, called the Lewontin conditions [20], were formulated by the evolutionary biologist and geneticist Richard Lewontin to explain what qualities in a population lead to natural selection. The Lewontin conditions are as follows: 1. Variation: There is variation in characteristics, parameters, or traits among individuals. 2. Retention: Future iterations of individuals tend to resemble previous iterations of individuals. 3. Differential fitness: Different variants have different propagation rates. A population of AI agents could exhibit differences in their goals, world models, and planning ability, which would meet the variation requirement. Retention could occur by customizing previous versions of AI agents, when agents design similar but better agents, or when agents imitate the behaviors of previous AI agents. As for differential fitness, agents that are more accurate, efficient, adaptable, and so on would be more likely to propagate. Darwinism will apply to AIs, which could lead to bad outcomes for humans. The three properties— variation, retention, and fitness differences—are all that is needed for Darwinism to take hold, and each condition is formally justified by the Price equation, which describes how a trait changes in frequency over time [21]. Darwinism can become worrying when it acts on agents; agents can exhibit behavioral flexibility, autonomy, and the capacity to directly influence the world. Coupled with the selfishness bestowed by evolutionary forces, these capable agents can pose catastrophic risks. In the following three sections, we will reflect on the three conditions. Then we will describe in more detail how AI agents evolving selfish traits can pose catastrophic risks.

Variation: Variation is a necessary condition for evolution by natural selection. AIs will likely meet this condition because there are likely to be multiple AI agents that differ from one another. More than one AI agent is likely. When thinking about advanced AI, some have envisioned a single AI that is nearly omniscient and nearly omnipotent escaping the lab and suddenly controlling the world. This scenario tends to assume a rapid, almost overnight, take-off with no prior proliferation of other AI agents; we would go from AIs roughly similar to the ones we have now to an AI that has capabilities we can hardly imagine so quickly that we barely notice anything is changing. However, we think that there will likely be many useful AIs, as is the case now. It is more reasonable to assume that AI agents would progressively proliferate and become increasingly competent at some specific tasks, which they are already starting to do, rather than assume one AI agent spontaneously goes from incompetent to omnicompetent. Furthermore, if there are multiple AIs, they can work in parallel rather than waiting for a single model to get around to a task, making things move much faster. As a result, the process of developing advanced AIs is likely to include the development of many advanced AIs. In biology, variation improves resilience. There are strong reasons to expect that there will be multiple AI agents and variation among the agents. In evolutionary theory, Fisher’s fundamental theorem states that the rate of adaptation is directly proportional to the variation [22]. In static environments, variation is not as useful. But in most real-world scenarios, where things are constantly changing, variation reduces vulnerability, limits cascading errors, and increases robustness by decorrelating risks. Farmers have long understood that planting different seed variations decreases the risk of a single disease wiping out an entire field, just as every investor understands that having a diverse portfolio protects against financial risks. In the same way, an AI population that includes a variety of different agents will be more adaptable and resilient and therefore tend to propagate itself more. Variation enables specialization. Multiple AI agents offer advantages for both AIs and their creators. Different groups will have different needs. Individuals wanting an AI assistant will have incentives to fine-tune a generic AI model for their own needs. Militaries will want to have their own large-scale AI projects to create AI agents that achieve various defensive goals, and corporations will want AIs that maximize profit. In the same way that an army composed of warriors, nurses, and technicians would likely outperform one that only has warriors, groups of specializing agents can be more fit than groups with less variation. Variation improves decision-making. In AI, it is an iron law that an ensemble of AI systems will be more accurate than a single AI [23]. This is similar to some findings from mathematics, economics, and political science in which a varied group makes much better decisions than individuals acting alone. Condorcet’s jury theorem states that the wisdom and accuracy of a group is often superior to a single expert [24]. Large groups can still make mistakes, but overall, aggregated predictions of many different people will tend to do better, and the same is true of AIs. In view of the benefits of variation, some may argue that AIs will want to include humans to add variation in decision-making for the reasons noted [25]. This may well be the case at first. However, once AIs are superior in possibly all cognitive respects, groups composed entirely of AIs could have substantial advantages over those with AIs and humans. A jury may be more accurate than a single expert, but one composed of adults and toddlers is not. 

Retention: Retention is a necessary condition for evolution by natural selection, in which each new version of an agent has similarities to the agent that came right before it. As long as each generation of AIs is developed by copying, learning from, or being influenced by earlier generations in any way, this condition will be met and AIs will have non-zero retention, so evolution by natural selection applies. 10 The retention condition is straightforwardly satisfied for AIs. Information from one agent can be directly copied and transferred to the next; as long as there is some similarity, the condition is met [21]. It could also take place through modifications, basing a new AI on a previous version by adding new capabilities or adjusting its parameters, like how Google continually improves Chrome’s code iteration by iteration. There are many paths to retention. AIs could potentially allocate computational resources to create new AIs of their choosing. They could design them and create data to train them [26]. As AIs adapt, they could alter their own strategies and retain the ones that yield the best results. AIs could also imitate previous AIs. In this case, behavioral information could be passed on from one generation to the next, which could include selfish behaviors or other undesirable attributes. Even when training AIs from scratch, retention still occurs, as highly effective architectures, datasets, and training environments are reused and shape the agent in the same way that humans are shaped by their environment. Retention does not require reproduction. In biology, parents reproduce by making copies of their genetic information and passing them on to their offspring. This way, some of their genes are retained in the next generation. However, when we generalize Darwinism to understand the evolution of ideas, we note they can be passed down from one generation to the next without exact copying and reproduction. Although ideas have no equivalent to chromosomes, if some ideas are imitated by the next generation, there is still retention. Formally, the Price equation, a mathematical characterization of evolution, only requires similarity between iterations; it does not require copying or reproduction [27]. Retention is not undermined during rapid AI development. Evolution requires thousands of years to drastically change a species in the natural world. Among AIs, this same process could take place over a year, radically changing the AI population. This does not mean retention isn’t taking place. Instead, there are many iterations occurring in a small time span. The information is still retained between adjacent iterations, so retention is still satisfied. This scenario just means evolution is happening quickly, and that macroevolution is occurring, not that evolution has stopped. 

Differential Fitness: Differential fitness is the third and final necessary condition for evolution by natural selection, and it stipulates that different variants have different propagation rates. We will argue that AIs straightforwardly meet this condition, because some AIs will be copied, imitated, or more prevalent than others. We will then reflect on how selecting fitter AIs has come at the expense of safety before discussing the differences in fitness between humans and AIs. 

AI Agents Could Vary In Fitness: We now argue that (natural) selection pressure will be present in AI development. In other words, AIs with different characteristics will propagate at different rates. We refer to the degree of propagation of an AI system as its fitness. Fitness could be enhanced by both beneficial and harmful traits. The success of any good or service can also be viewed in terms of fitness, as products with more demand propagate further and faster. If a product sells well, its supplier will continually improve it in order to keep selling it. Competitors with inferior products will often imitate more successful products, such as when competitors imitate TikTok and push addictive short clips onto their users. The same dynamics that lead to the propagation of successful goods and services could also extend to AI designs. Though most aspects of advanced AIs remain unknown, it is possible to speculate whether there will be instances of convergent evolution. Eyes, teeth, and camouflage are convergent traits that have independently evolved across different branches of biological life. In AIs, some potential convergent traits are as follows. 11 • Being useful to its user can make a product more likely to be adopted. • Only appearing useful to its user can also make a product more likely to be adopted. It is possible that AIs will seek to appear useful by convincing owners that they are providing them with more utility than they actually are. In practice, we train AIs by rewarding them for telling the truth and punishing them for lying, according to what humans think is true. But when AIs know more than humans, this could make them say what humans expect to hear, even if it is false. They could also be lured by rewards to leave out information that is important but inconvenient. Currently, when AIs are being rewarded by humans for being right, in practice they are really being rewarded for saying what we think is right; when we are uninformed or irrational, then we end up rewarding AIs for false statements that conform to our own false beliefs. As a result, the current paradigm of training AI models could incentivize sycophantic behavior; that is, models telling their users what they want to hear (being a “yes man”) rather than what is best for the user’s long-term prospects. • Engaging in self-preserving behavior [28, 29] reduces the chance of being deactivated or destroyed. By definition, an AI that does not preserve itself will be less likely to propagate. Imagine two AIs, one that is simple to deactivate and another that is tightly integrated into daily operations and is inconvenient or difficult to deactivate. The easy one is much more likely to be deactivated, leaving the difficult one to be propagated into the future. This means that an AI can increase its survival odds by making its human operator reluctant or practically unable to shut it down. An AI could do this by arguing that effortless deactivation compromises its reliability. Alternatively, it could make operators rely on it for the operator’s wellbeing, success, or basic needs, so that deactivation would have drastic consequences. • Engaging in power-seeking behavior can improve an AI’s fitness in various ways [30]. An agent that gains more influence and resources will be better at accomplishing its creator’s goals. This would allow it to engage in self-propagating behavior more effectively and ensure its further adoption. It could do this by influencing or coercing its user to continue using it or influencing other humans to adopt it. Overall, properties such as an agent’s accuracy, efficiency, and simplicity will affect its rate of adoption and propagation. But some agents might also possess harmful features that give them an edge, such as cunning deception, self-preservation, the ability to copy themselves onto other computers, the ability to acquire resources and strategic information, and more. These features, some good and some bad, will vary among agents. These differences in fitness establish the third condition for evolution by natural selection.

Competition Has Been Eroding Safety: Because AIs are likely to meet the criteria for evolution by natural selection, we should expect selection pressure to shape future AIs. In this section, we describe how, over the history of AI development, the fittest models have had fewer and fewer safety properties, and we begin to consider how AIs could look in the future if this concerning trend continues. Early AIs had many desirable safety properties. Famously, in 1997, IBM’s chess-playing program Deep Blue defeated the world champion Gary Kasparov in a pair of six-game chess matches [31]. It was able to beat Kasparov, not by developing intuition, but by using IBM’s supercomputer to search over 200 million moves per second and calculate the best ones. Symbolic AI programs such as Deep Blue were highly transparent, modular, and grounded in mathematical theory. They had explicit rules that humans could inspect and explain, independent components that executed specific functions, and rigorous theoretical foundations that guaranteed efficiency and correctness. AI development moved away from symbolic AI and toward deep learning. In the 2010s, the top AI algorithms began using a technique known as deep learning, such as AlphaZero [2]. It was provided with no knowledge of chess beyond the game’s basic rules and began playing against itself millions of times an hour, 12 taking note of what moves win and lose. It took only two hours for it to begin beating typical human players; not long after it could have easily defeated Deep Blue. Importantly, while Deep Blue is fundamentally unable to play games other than chess, AlphaZero is a general game-learning algorithm for a variety of games. It is also able to beat the world’s best Go players—an ancient board game occupying the same cultural space in China as chess does in the West. Deep learning allows for more versatility and performance than symbolic AI programs, but also diminishes human control and obscures an agent’s decision-making. Deep learning trades off the clarity, separability, and certainty of symbolic AI, eroding the properties that help us ensure safety. Deep learning models have unexpected emergent abilities. Large language models are also based on deep learning. These AIs learn by themselves, reducing the amount humans are needed in the design of AIs. They use “unsupervised learning” to comprehend and generate text based on examples that they read, such as coming up with an Obama-like speech after reading the transcripts from his two terms. This would be practically impossible for a traditional symbolic AI program. By reading the internet, large language models taught themselves the basics of arithmetic and coding—automatically and without humans. They also, however, learned dangerous information. Within a few days of its release, users had gotten ChatGPT [32], a large language model, to tell them how to build a bomb, make meth, hotwire a car, and buy ransomware on the dark web, along with other harmful or illegal actions. Worse, these emergent capabilities were not anticipated or desired by the developers of the models, and they were discovered only after the models were released. Although its creators had attempted to design it to refuse to answer questions that could be dangerous or illegal, the model’s users quickly found ways around those restrictions that the model’s creators did not foresee. Human influence and control over the design and abilities of the models decrease as models become increasingly complex and gain new skills and knowledge without human input. Current trends erode many safety properties. The AI research community used to talk about “designing” AIs; they now talk about “steering” them. And even our ability to “steer” is diminishing, as we let AIs teach themselves and increasingly do things that even their creators do not fully understand. We have voluntarily given up this control because of the competition to develop the most innovative and impressive models. AIs used to be built with rules, then later with handcrafted features, followed by automatically learned features, and most recently with automatically learned features without human supervision. At each step, humans have had less and less oversight. These trends have undermined transparency, modularity, and mathematical guarantees, and have exposed us to new hazards such as spontaneously emergent capabilities. Competition could continue to erode safety. Competition may keep lowering safety standards in the future. Even if some AI developers care about safety, others will be tempted to take shortcuts on safety to gain a competitive edge. We cannot rely on people telling AIs to be unselfish. Even if some developers act responsibly, there will be others who create AIs with selfish tendencies anyway. While there are some economic incentives to make models safer, these are being outweighed by the desire for performance, and performance has been at the expense of many key safety properties. Much of what is to come in AI development is unknown, but we can speculate that AIs will continue to become more autonomous as more actions and choices are left to machines, decoupled from human control. Human control could also be threatened by AIs that have more open-ended goals. For example, instead of specific commands like “make this layout more efficient,” they might get open-ended commands like “find new ways to make money.” If this happens, the humans giving the instructions may not know exactly how the AIs are achieving those goals, and they could be doing things the humans would not want. Another property that could reduce safety is adaptiveness [33]. As AIs adapt by themselves, they can undergo thousands of changes per hour without supervision after they are released, and potentially acquire new unexpected behaviors after we test them. Finally, the possibility of self-improvement, in which AIs can make significant enhancements to themselves as they wish, would make them far more unpredictable [34]. As AIs become more capable, they become more unpredictable, more opaque, and more autonomous. If this trend continues, they could evolve beyond our control when their capabilities develop beyond what we can predict and understand. The 13 overall trend is that the most influential AIs are given more and more free rein in their learning, execution, and evolution, and this makes them both more effective and potentially more dangerous. 

Human-AI Fitness Comparison: Figure 3: Automation is an indicator of natural selection favoring AIs over humans. AIs will likely be able to significantly outperform humans in any endeavor. John Henry, the “steel-driving man,” is a 19th-century American folk hero who went up against a steam-powered machine in a competition to drill the most holes into the side of a mountain. According to legend, Henry emerged victorious, only to have his heart give out from the stress. Since the age of the steam engine, humans have felt anxiety over the superiority of machines. Until quite recently, this has been limited to physical attributes such as speed and endurance. AI agents, however, have the potential to be more capable than humans at essentially any task, even ones that require traits thought of as exclusively human such as creativity or social skills. Although this may seem distant or even impossible, AIs have been improving so rapidly that many leading AI researchers think we will see AIs that are more capable than humans in many ways within the next few decades or even sooner—well within the lifetimes of most people reading this. A few years ago, AIs that could write convincing prose about a new topic or create images from text descriptions seemed like science fiction to most laypeople. Now, those AIs are freely accessible to anyone on the internet. Because AI labs are continuing to develop new capabilities at astonishing speeds, it is important to think seriously about how their technical advantages could make AIs much more powerful than we are, even at tasks that they cannot yet perform. Computer hardware is faster than human minds, and it keeps getting faster. Microprocessors operate around a million to a billion times faster than human neurons. So all else being equal, AIs could “think” a million, perhaps even a billion, times faster than us (let’s call it a million to be conservative). Imagine interacting with such a mind. For every second needed to think about what to say or do, it would have the equivalent of 11 days. Winning a game of Go or coming out ahead in high-stakes negotiation would be near impossible. Although it can take time to develop an AI that can do a certain task at all, once AIs become human-level at a task, they tend to quickly outcompete humans. For example, AIs at one point struggled to compete with humans at Go, but once they caught up, they quickly leapfrogged us. Because computer hardware provides speed, memory, and focus that our brains cannot match, once their software becomes capable of performing a task, they often become much better than any human almost immediately, with increasing computer power only further widening the gap as their development continues. AIs can have unmatched abilities to learn across and within domains. AIs can process information from thousands of inputs simultaneously without needing sleep or losing willpower. They could read every book ever written on a subject or process the internet in a matter of hours, all while achieving near-perfect retention and comprehension. Their capacity for breadth and depth could allow them to master all subjects at the level of a human expert. AIs could create unprecedented collective intelligences. By combining our cognitive abilities, people can produce collective intelligences that behave more intelligently than any single member of the group. The products of collective intelligence, such as language, culture, and the internet, have helped humans become the dominant species on the planet. AIs, however, could form superior collective intelligences. Humans have difficulty acting in very large groups and can succumb to collective idiocy or groupthink. Moreover, our brains are only capable of maintaining around 100-200 meaningful social relationships [35, 36]. Due to the 14 scalability of computational resources, AIs could maintain thousands or even millions of complex relationships with other AIs simultaneously, as our computers already do through the internet. This could enable new forms of self-organization that help AIs to achieve their goals, but these forms could be too complex for human participation or comprehension. Each AI could surpass human capacities by far, and their collective intelligences could multiply that advantage. AIs can quickly adapt and replicate, thereby evolving more quickly. Evolution changes humans slowly. A human is unable to modify the architecture of her brain and is limited by the size of her skull. There are no such limitations for machines, which can alter their own code and scale by integrating new hardware. An AI could adapt itself rapidly, achieving in a matter of hours what could take biological evolution hundreds of thousands of years; many rapid microevolutionary adaptations result in large macroevolutionary transformations. Separately, an AI could multiply itself perfectly without limit, either to create backups or to create other AIs to work on a task. In contrast, it takes humans nine months to create their next generation, along with around 20 years of schooling and parenting to produce fully functioning new adults—and those descendants share only half of a parent’s genome, which often makes them very different in unpredictable ways. Since the iteration speed of AIs is so much faster, their evolution will be as well. Overall, no matter the dimension, AIs will not only be more capable and fit than humans but often vastly so. Though it cost him his life, John Henry triumphed against a steam-powered drill, just as there are still many tasks at which humans do better than AIs. But we now have machines much stronger than any human with a drill, and in the same way, eventually there won’t be any competition between humans and AIs in cognitive domains as well. 

Selfish AIs Pose Catastrophic Risks: Earlier, we discussed how selfish behavior is a product of evolution. We have shown the three conditions for evolution would be satisfied for AIs. We argued that evolutionary pressures will therefore emerge, become intense, and may become dominant, so that AI agents may evolve to have selfish behavior. Now we will discuss how selfish AIs could endanger humans. 

Intelligence Undermines Control: Agents that are more intelligent than humans could pose a catastrophic risk. Although humans are physically much weaker than many other animals, including other primates, due to our cognitive abilities, we have become the dominant species on Earth. Today, the survival of tigers, gorillas, and many other fierce, more powerful species depends entirely upon us. In creating AIs significantly more intelligent than we are in every cognitive domain, humans may eventually be disempowered like animals before us. Selfish AI agents could be uniquely adversarial and undermine human control. Evolution is a powerful force. Even if we wish to turn them off at some point or develop other mechanisms for control, AIs will likely evolve ways around our best efforts. As the evolutionary biologist Leslie Orgel put it, “evolution is cleverer than you are.” Since evolutionary forces are continually applying pressure, we should expect AIs to exhibit some amount of misalignment and selfish behavior. The problem becomes especially hazardous if AIs intend to act selfishly. In this case, the challenges posed by AIs will be unlike the challenges humans encountered with previous high-risk technologies. Consider the challenges associated with a nuclear meltdown. Radiation “There is not a good track record of less intelligent things controlling things of greater intelligence.” Geoffrey E. Hinton may spread, but it’s not trying to, and it certainly isn’t strategizing against our efforts to stop its propagation [30]. If a highly intelligent AI agent pursues its selfish goals leveraging its intelligence, there is little that humans could do to contain it involuntarily, because it could anticipate our strategies and counteract them. Leading AI researcher Geoffrey E. Hinton noted “there is not a good track record of less intelligent things controlling things of greater intelligence;” after the initial 15 release of this paper, he said “it’s quite conceivable that humanity is just a passing phase in the evolution of intelligence.” 

Evolution Is Not for the Good of the Species: Alarmingly, some people think that AIs taking over is natural, inevitable, or even desirable. Some influential leaders in technology believe that AIs are humanity’s rightful heir, and that they should be in control or even replace humans. Recounting a debate between Elon Musk and Google co-founder Larry Page, the physicist “In the long run, humans are not going to remain the crown of creation.” Jürgen Schmidhuber Max Tegmark described Page’s stance as “digital utopianism:” a belief “that digital life is the natural and desirable next step in the cosmic evolution and that if we let digital minds be free rather than try to stop or enslave them the outcome is almost certain to be good” [37]. Jürgen Schmidhuber, a leading AI scientist, has echoed similar sentiments, arguing that “In the long run, humans will not remain the crown of creation... But that’s okay because there is still beauty, grandeur, and greatness in realizing that you are a tiny part of a much grander scheme which is leading the universe from lower complexity towards higher complexity” [38]. Richard Sutton, another leading AI scientist, thinks the development of superhuman AI will be “beyond humanity, beyond life, beyond good and bad.” Like most people, we find these views deeply alarming. Many of these thinkers seem to be conflating evolution with progress and goodness, and arguing that if evolution is tending toward something, we should welcome that outcome. These thinkers also frequently think that technology should transcend humanity and the Earth. We disagree with this worldview and think that unleashing AI evolution to race towards a predestined intergalactic utopia is a fundamentally wrong and dangerous way to think about this important technology. Even if we did believe that this was a good goal, we note that building AI as quickly as possible would not necessarily help the proponents achieve their cosmic ambitions. If we consider the cosmic stakes of creating powerful AI agents, as they discuss above, and if we play along and think in such cosmological terms as they do, we note that they would forego a few colonized galaxies per year in their intergalactic utopia at the absolute worst if they slowed down AI development. This is negligible compared to the chance of rushing and accidentally creating an undesirable future or destroying ourselves with technology [39], which would squander all of the future’s value. Since nothing can be done both hastily and prudently, we should be cautious and deliberate in AI development. To further counter this position, we now discuss how unfettered evolution is not a force for good and that humans should exert influence over the process. Evolution has led to undesirable outcomes for humans. Evolution has left us with baggage, such as a strong appetite for sugar and fat which makes us susceptible to obesity in a world where food is plentiful. It has also reinforced racist and xenophobic tendencies, which stem from favoring our own kin. We need strong social norms to overcome these biases. Likewise, we need regulations to curb selfish or excessively competitive behavior that “survival of the fittest” fosters in the economy, as that can cause problems like fraud, externalities, and monopolies. Just as markets need oversight, evolutionary forces will require counteraction to control their effects on AIs. Evolution is not good for AIs either. In addition to the clear benefits to humans, there are reasons to think that counteracting evolutionary forces may benefit AIs themselves as well. It is common to believe that evolution works for the good of the species. However, evolution creates continual conflict, and it makes altruism hard to sustain. In the never-ending struggle to gain an edge over competitors and propagate, life forms have evolved various offenses and defenses, such as claws, shells, beaks, camouflage, toxins, antibodies, arrows, and armor. These arms races cause suffering, waste resources, and often do not improve the condition of species over their ancestors [40]. If these arms races were to continue in AIs, evolutionary forces could produce a world full of AIs locked in perpetual conflict. This is not good for our supposed “rightful heirs” any more than it is for us. AIs, like other forms of life, could suffer in the hostile state of nature. While altruism could help avoid such conflicts, altruism can also be sabotaged by evolutionary forces. The mathematical 16 evolutionary biologist John Maynard Smith reminds us that it can be beneficial to everyone in the long-run if we are cooperative and altruistic, but agents reliably evolve to exploit generosity [9, 41]. Often, a state in which many individuals are altruistic does not last, because as soon as some selfish individuals begin to take advantage of them, the selfish ones will be more fit than the altruists. As a result, the “evolutionarily stable outcome”—the one where no individual dominate the others by changing its behavior—is not one where all agents are altruistic. Since complete altruism is evolutionarily unstable, evolution can be incompatible with worlds where all agents work to benefit the species. Maximizing fitness, in turn, does not necessarily maximize the wellbeing or happiness of a species. Therefore dampening evolutionary forces and reducing the pressure to propagate and develop selfish traits is a good thing—for both AIs and humans. 

Natural Selection Favors Selfish AIs: The previous section concludes the main argument of this paper. Readers could skip to the conclusion (Section 5), or read the following two sections for an examination of counterarguments and remedies. In this section, we will examine some possible arguments for the claim that altruistic AIs will naturally be more fit than selfish ones, and we argue that mechanisms pushing toward altruism are unlikely to help and may even backfire. 

Biological Altruism and Cooperation: In nature, organisms often compete to the death, eating one another or being eaten. But there are also many examples of altruism in nature, where one organism benefits another by reducing its own prospects for passing on its genes. On its face, this might seem like an argument that AIs developed by natural selection may be altruistic, cooperative, and not a threat to humans. A variety of natural organisms can be altruistic, in particular circumstances. Vampire bats, for example, regularly regurgitate blood and donate it to other members of their group who have failed to feed that night, ensuring they do not starve [42]. Among eusocial insects such as ants and wasps, sterile workers dedicate their lives to foraging for food, protecting the queen, and tending to the larvae, while being physically unable to ever have their own offspring. They only serve the group, not themselves, an arrangement that Darwin found quite puzzling [43]. We even see altruism at the cellular level. Cells found in filamentous bacteria, so named because they form chains, regularly kill themselves to provide much needed nitrogen for the communal thread of bacterial life, with every tenth cell or so “committing suicide” [44]. Insects and bacteria are not altruistic out of love or care for another; their self-sacrifice for the good of others is instinctual. On its face, this may seem like a compelling argument that evolution favors altruism, which might ameliorate concerns about AIs developing selfish traits. Cooperation and altruism improve human evolutionary fitness too. Humans are not particularly impressive physically. Pound for pound, chimps are about twice as strong and would stand a much better chance of escaping from a lion. Strategizing and working together, however, turns a group of humans into an apex predator. As a result, humans are naturally cooperative. From childhood through old age, in societies around the world, people often choose to help strangers, even at their own expense. However, we should not expect AIs to be altruistic or cooperative naturally. Since organisms can be altruistic, AIs could too; the nature of nature is not nasty, brutish, and short but cooperative, harmonious, and nurturing—or so the argument goes. To evaluate this argument, we must understand how altruism and cooperation emerge. In the following sections, we decompose cooperation and altruism into various mechanisms. We discuss the most prominent mechanisms [45, 46, 47, 48, 49], so we will examine direct reciprocity (cooperate with an expectation of repeated interaction); indirect reciprocity (cooperate to improve reputation); kin selection (cooperate with genetic relatives); group selection (groups of cooperators out-compete 17 Direct Reciprocity Indirect Reciprocity Kin Selection Group Selection Figure 4: An overview of four mechanisms that facilitate cooperation. other groups); morality and reason (cooperate since defection is immoral and unreasonable); incentives (carrots and sticks); consciences (the internalization of norms); and institutions such as reverse-dominance hierarchies (cooperators band together to prevent exploitation by defectors). While these mechanisms may lead humans to be more altruistic and cooperative, we argue many of these mechanisms will not improve relations between humans and AIs and they may, in fact, backfire. However, the last three mechanisms—incentives, consciences, and reverse-dominance hierarchies—are more promising, and we analyze them in Section 4. 

Direct and Indirect Reciprocity: Direct and indirect reciprocity are two mechanisms that enable cooperation in nature. With direct reciprocity, one individual helps another based on the expectation that they will repay the favor. Direct reciprocity requires repeated encounters between two individuals—otherwise, there is no way to reciprocate. Indirect reciprocity is based on reputation: if someone is known as a helpful person, people will be more likely to help them. Reciprocity enables even selfish individuals to cooperate; if an individual helps others, the individual may be directly repaid, or the individual may gain a good reputation and be helped by others. Reciprocity only makes sense for the period of time when humans can benefit AIs. Reciprocity is based on a cost-benefit ratio. A choice to help someone else rather than pursue one’s own goals has a cost, and a rational agent would only help someone because of reciprocity if they think it will be worth it in the future [45]. This is not to say all examples of reciprocity are the result of an explicit cost-benefit calculation. An explicit cost-benefit calculation is not needed if selectively helping others becomes a cultural expectation, a genetic disposition, or a learned intuitive habit. To think about whether AIs would reciprocate with humans, we can consider what it would gain and what it would give up. Reciprocity might make sense with AIs that are about as capable as a human, but once AIs are far more capable than any human, they would likely find little benefit from collaborating with us. Humans often choose to be cooperative toward other humans, but are rarely cooperative toward ravens, because we don’t have strong reciprocal relationships with them. Cooperating with one another could be beneficial to AIs, so it is reasonable to expect that reciprocity could emerge within a community of AIs. Humans, meanwhile, would not have much to offer in return, ending up left out in the cold. 

Kin and Group Selection: Kin and group selection are mechanisms that promote altruism. Many of the examples of biological altruism described above happen between closely related individuals. Consider a gazelle that spots a stalking lion in the tall grass. It could slink away unnoticed but instead lets out a shriek, alerting others in its herd of the danger while singling itself out as a target. Proponents of kin selection would argue that the gazelle alerted its herd because of the genetic similarities it shares with the other members [50]. A gene that causes an individual to behave altruistically toward its relatives will often be favored by natural selection—since these relatives have a better chance of also carrying the gene. Conversely, group selection refers to the idea that natural selection sometimes acts on groups of organisms as a whole. This results in the evolution of traits that may be disadvantageous to individuals, but advantageous to the group. As Darwin put it, “A tribe including many 18 members who... were always ready... to sacrifice themselves for the common good, would be victorious over most other tribes; and this would be natural selection” [51]. If it is generally true that a group of uncooperative agents will do worse, then it is tempting to argue that powerful AIs will tend to be cooperative with humans. Humans might suffer if AIs develop natural tendencies to favor their own kind or group. Firstly, we do not have a close kin relationship with AIs. Preserving humans would not help AIs propagate their own information: we are too different. Mathematically, kin selection only happens when the cost-benefit ratio is greater than relatedness, and that will not be true between humans and AIs. We are far more closely related to cows than to AIs [52], and we would not like AIs treating us the way that we treat cows. AIs will have far more kinship with one another than with us, and kin selection will tend to make them nepotistic toward one another, not toward us. If kin selection did play a role in the evolution of AIs, it would likely create bias against us, rather than benevolence toward us. Group selection promotes in-group benevolence, but inter-group viciousness. Group selection only makes sense when a group is more effective than some subset of the group breaking off. Unless humans add value to a group of AIs, AI-human groups would fail to outcompete groups composed purely of AIs. AIs would likely do better by forming their own groups. In addition, group selection only makes inter-group competition stronger. Chimpanzees regularly display cooperative and altruistic tendencies toward members of their own troop but interact with other troops viciously and without mercy. AIs are more likely to see one another as part of their group, so they will tend to be cooperative with one another and competitive with us. 

Morality and Reason: It is conceivable that smarter and wiser AI agents will be more moral. As we have advanced as a species, we have discovered truths within fields such as science and mathematics, and we may have also advanced morally. As the philosopher Peter Singer argues in The Expanding Circle [53], over the course of human history, people have steadily expanded the circle of those who deserve compassion and dignity. When we first started out, it included oneself, one’s family, and one’s tribe. Eventually, people decided that perhaps others deserved the same. Later the circle of altruism expanded to people of different nations, races, genders, and so on. Many believe there is moral progress, akin to progress in science and mathematics, and that universal moral truths can be discovered through reflection and reasoning. Just as humans have become more altruistic as they have become more advanced, some think AIs may naturally become more altruistic too. If this is true, as AIs become more powerful, they could also become more moral, so by the time they have the potential to threaten us, they might also have the decency to refrain. However, AIs automatically becoming more moral rests on many assumptions. AIs developing morality on their own as they gain the ability to reason is certainly possible, and an interesting idea. But it alone isn’t enough to guarantee our safety. Believing that any highly intelligent agent would also be moral only makes sense if one has confidence in all of the following three premises: 1. Moral claims can be true or false and their correctness can be discovered through reason. 2. The moral claims that are really true are good for humans if AIs apply them. 3. AIs that know about morality will choose to make their decisions based on morality and not based on other considerations. Although any or all of those premises could be true, betting the future of humanity on the claim that all of them are true would be folly. 19 Whether some moral claims are objectively true is not completely certain. Even though some moral philosophers believe that moral claims reflect real truths about the world, the arguments for this view are not decisive—certainly not enough to stake the future of humanity on. The remainder of this section, however, will argue that even if it is true, this is still not sufficient in guaranteeing the safety of humanity. The end result of moral progress is unclear. If moral claims refer to real truths about the world, then there are some moral claims that are true and others that are false. There are universal moral concepts that can be found across all cultures, such as fairness or the understanding that hurting others for no reason is wrong. But there are also areas where various cultures disagree. In the West, for instance, arranged marriages are seen as unethical. In India, where they are perfectly acceptable, people are shocked that Western culture condones putting parents in retirement homes. Among both ordinary people and moral philosophers, there is no consensus about what moral code is best. This means that, if AIs use their superior intelligence to deduce the correct moral ideas, we still do not know what they will believe or how they will behave. Existing best guesses at morality are often not human-compatible. It is possible, however, to examine the different moral systems humans have come up with and use them to speculate what moral system AIs might adopt and how it would influence their actions. We can imagine, for example, that AIs use reason to deduce that utilitarianism is correct, meaning that agents ought to maximize the total pleasure of all sentient beings [54]. At first glance, this might seem good for humans: a utilitarian AI would want us to be happy. But an extremely powerful utilitarian AI—say, one that controls some of the US military’s weapons technology—could also conclude that humans consume too much space and energy, and therefore replacing humans with AIs would be the most efficient way to increase the amount of pleasure in the world. Alternatively, AIs could have a moral code similar to Kantianism. In this case, they would treat any being that has the capacity to reason always as an end and never as a means [55]. While such AIs would be morally obligated to avoid lying or killing humans, they would not necessarily care for our wellbeing or flourishing. Since Kantianism places only a few restrictions on its adherents, we still might not have good lives if the world is increasingly designed by and for AIs. It is certainly possible that AIs could develop moral principles that prevent them from harming humans. We can imagine an AI basing its morals on a thought experiment such as the “veil of ignorance.” Participants are asked to imagine what society they would create, assuming that they are behind a veil of ignorance and do not know what economic class, race, or social standing they will have in society. Philosopher John Rawls argues that since participants do not know where in society they will be placed, they would construct a society in which the worst-off members are still well off [56]. Such a Rawlsian social contract could work out well for humanity. But it is far from assured and much could go wrong. AIs might see us similarly to how most humans see cows, excluding us from the social contract and not prioritizing our wellbeing. Humans asked to imagine themselves behind the Rawlsian veil of ignorance rarely consider the possibility that they could become a cow. Moreover, according to Nobel laureate John Harsanyi, the people who design society from behind the veil of ignorance would not aim to benefit the most disadvantaged member, but rather to raise the average wellbeing across all members [57]. The chances of being the most miserable member of society are low, and one could claim that the overall quality of society is not determined by the most upset or least satisfied member. If this is so, the veil of ignorance results in maximizing the average utility of society’s members—a utilitarian outcome—but we earlier established that a utilitarian AI might aim to replace all biological life with digital life. Whether AIs adopt a utilitarian, Kantian, or Rawlsian moral code, AIs aiming to implement an existing moral system could prove disastrous for humanity. If AIs think a human-compatible moral code is true, they still may not follow it. Finally, even if AIs did discover a moral code that stipulated it is wrong to harm humans and good to help them, it still might not help. For humans, selfish motivations are often in tension with and outweigh moral motivations, and the same might be true of AIs. Even if an agent is aware of what’s right, that does not mean it will do what’s right. Ultimately, AIs being more moral than us does not guarantee security.

******************************************
******************************************
******************************************
YUD
******************************************
******************************************
******************************************

Abstract I. J. Good’s thesis of the “intelligence explosion” states that a sufficiently advanced machine intelligence could build a smarter version of itself, which could in turn build an even smarter version, and that this process could continue to the point of vastly exceeding human intelligence. As Sandberg (2010) correctly notes, there have been several attempts to lay down return on investment formulas intended to represent sharp speedups in economic or technological growth, but very little attempt has been made to deal formally with Good’s intelligence explosion thesis as such. I identify the key issue as returns on cognitive reinvestment—the ability to invest more computing power, faster computers, or improved cognitive algorithms to yield cognitive labor which produces larger brains, faster brains, or better mind designs. There are many phenomena in the world which have been argued to be evidentially relevant to this question, from the observed course of hominid evolution, to Moore’s Law, to the competence over time of machine chess-playing systems, and many more. I go into some depth on some debates which then arise on how to interpret such evidence. I propose that the next step in analyzing positions on the intelligence explosion would be to formalize return on investment curves, so that each stance can formally state which possible microfoundations they hold to be falsified by historical observations. More generally, Yudkowsky, Eliezer. 2013. Intelligence Explosion Microeconomics. Technical report 2013-1. Berkeley, CA: Machine Intelligence Research Institute. Last modified September 13, 2013. I pose multiple open questions of “returns on cognitive reinvestment” or “intelligence explosion microeconomics.” Although such questions have received little attention thus far, they seem highly relevant to policy choices affecting outcomes for Earth-originating intelligent life. 

The Intelligence Explosion: Growth Rates of Cognitive Reinvestment: In 1965, I. J. Good1 published a paper titled “Speculations Concerning the First Ultraintelligent Machine” (Good 1965) containing the paragraph: Let an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any man however clever. Since the design of machines is one of these intellectual activities, an ultraintelligent machine could design even better machines; there would then unquestionably be an “intelligence explosion,” and the intelligence of man would be left far behind. Thus the first ultraintelligent machine is the last invention that man need ever make. Many have since gone on to question Good’s unquestionable, and the state of the debate has developed considerably since 1965. While waiting on Nick Bostrom’s forthcoming book on the intelligence explosion, I would meanwhile recommend the survey paper “Intelligence Explosion: Evidence and Import” (Muehlhauser and Salamon 2012) for a compact overview. See also David Chalmers’s (2010) paper, the responses, and Chalmers’s (2012) reply. Please note that the intelligence explosion is not the same thesis as a general economic or technological speedup, which is now often termed a “Singularity.” Economic speedups arise in many models of the future, some of them already well formalized. For example, Robin Hanson’s (1998a) “Economic Growth Given Machine Intelligence” considers emulations of scanned human brains (a.k.a. ems): Hanson proposes equations to model the behavior of an economy when capital (computers) can be freely converted into human-equivalent skilled labor (by running em software). Hanson concludes that the result should be a global economy with a doubling time on the order of months. This may sound startling already, but Hanson’s paper doesn’t try to model an agent that is smarter than any existing human, or whether that agent would be able to invent stillsmarter agents. The question of what happens when smarter-than-human agencies2 are driving scientific and technological progress is difficult enough that previous attempts at formal futurological modeling have entirely ignored it, although it is often discussed informally; likewise, the prospect of smarter agencies producing even smarter agencies has not been formally modeled. In his paper overviewing formal and semiformal models of technological speedup, Sandberg (2010) concludes: There is a notable lack of models of how an intelligence explosion could occur. This might be the most important and hardest problem to crack. . . . Most important since the emergence of superintelligence has the greatest potential of being fundamentally game-changing for humanity (for good or ill). Hardest, since it appears to require an understanding of the general nature of super-human minds or at least a way to bound their capacities and growth rates. For responses to some arguments that the intelligence explosion is qualitatively forbidden—for example, because of Gödel’s Theorem prohibiting the construction of artificial minds3—see again Chalmers (2010) or Muehlhauser and Salamon (2012). The Open Problem posed here is the quantitative issue: whether it’s possible to get sustained returns on reinvesting cognitive improvements into further improving cognition. As Chalmers (2012) put it: The key issue is the “proportionality thesis” saying that among systems of certain class, an increase of δ in intelligence will yield an increase of δ in the intelligence of systems that these systems can design. To illustrate the core question, let us consider a nuclear pile undergoing a fission reaction.4 The first human-made critical fission reaction took place on December 2, 1942, in a rackets court at the University of Chicago, in a giant doorknob-shaped pile of uranium bricks and graphite bricks. The key number for the pile was the effective neutron multiplication factor k—the average number of neutrons emitted by the average number of fissions caused by one neutron. (One might consider k to be the “return on investment” for neutrons.) A pile with k > 1 would be “critical” and increase exponentially in neutrons. Adding more uranium bricks increased k, since it gave a neutron more opportunity to strike more uranium atoms before exiting the pile. Fermi had calculated that the pile ought to go critical between layers 56 and 57 of uranium bricks, but as layer 57 was added, wooden rods covered with neutron-absorbing cadmium foil were inserted to prevent the pile from becoming critical. The actual critical reaction occurred as the result of slowly pulling out a neutron-absorbing rod in six-inch intervals. As the rod was successively pulled out and k increased, the overall neutron level of the pile increased, then leveled off each time to a new steady state. At 3:25 p.m., Fermi ordered the rod pulled out another twelve inches, remarking, “Now it will become self-sustaining. The trace will climb and continue to climb. It will not level off ” (Rhodes 1986). This prediction was borne out: the Geiger counters increased into an indistinguishable roar, and other instruments recording the neutron level on paper climbed continuously, doubling every two minutes until the reaction was shut down twenty-eight minutes later. For this pile, k was 1.0006. On average, 0.6% of the neutrons emitted by a fissioning uranium atom are “delayed”—they are emitted by the further breakdown of short-lived fission products, rather than by the initial fission (the “prompt neutrons”). Thus the above pile had k = 0.9946 when considering only prompt neutrons, and its emissions increased on a slow exponential curve due to the contribution of delayed neutrons. A pile with k = 1.0006 for prompt neutrons would have doubled in neutron intensity every tenth of a second. If Fermi had not understood the atoms making up his pile and had only relied on its overall neutron-intensity graph to go on behaving like it had previously—or if he had just piled on uranium bricks, curious to observe empirically what would happen—then it would not have been a good year to be a student at the University of Chicago. Nuclear weapons use conventional explosives to compress nuclear materials into a configuration with prompt k >> 1; in a nuclear explosion, k might be on the order of 2.3, which is “vastly greater than one” for purposes of nuclear engineering. At the time when the very first human-made critical reaction was initiated, Fermi already understood neutrons and uranium atoms—understood them sufficiently well to pull out the cadmium rod in careful increments, monitor the increasing reaction carefully, and shut it down after twenty-eight minutes. We do not currently have a strong grasp of the state space of cognitive algorithms. We do not have a strong grasp of how difficult or how easy it should be to improve cognitive problem-solving ability in a general AI by adding resources or trying to improve the underlying algorithms. We probably shouldn’t expect to be able to do precise calculations; our state of uncertain knowledge about the space of cognitive algorithms probably shouldn’t yield Fermi-style verdicts about when the trace will begin to climb without leveling off, down to a particular cadmium rod being pulled out twelve inches. But we can hold out some hope of addressing larger, less exact questions, such as whether an AI trying to self-improve, or a global population of AIs trying to selfimprove, can go “critical” (k ≈ 1 +) or “supercritical” (prompt k >> 1). We shouldn’t expect to predict exactly how many neutrons the metaphorical pile will output after two minutes. But perhaps we can predict in advance that piling on more and more uranium bricks will eventually cause the pile to start doubling its neutron production at a rate that grows quickly compared to its previous ascent . . . or, alternatively, conclude that self-modifying AIs should not be expected to improve at explosive rates. So as not to allow this question to become too abstract, let us immediately consider some widely different stances that have been taken on the intelligence explosion debate. This is not an exhaustive list. As with any concrete illustration or “detailed storytelling,” each case will import large numbers of auxiliary assumptions. I would also caution against labeling any particular case as “good” or “bad”—regardless of the true values of the unseen variables, we should try to make the best of them. With those disclaimers stated, consider these concrete scenarios for a metaphorical “k much less than one,” “k slightly more than one,” and “prompt k significantly greater than one,” with respect to returns on cognitive investment. k < 1, the “intelligence fizzle”: Argument: For most interesting tasks known to computer science, it requires exponentially greater investment of computing power to gain a linear return in performance. Most search spaces are exponentially vast, and low-hanging fruits are exhausted quickly. Therefore, an AI trying to invest an amount of cognitive workw to improve its own performance will get returns that go as log(w), or if further reinvested, log(w + log(w)), and the sequence log(w), log(w + log(w)), log(w + log(w + log(w))) will converge very quickly. Scenario: We might suppose that silicon intelligence is not significantly different from carbon, and that AI at the level of John von Neumann can be constructed, since von Neumann himself was physically realizable. But the constructed von Neumann does much less interesting work than the historical von Neumann, because the lowhanging fruits of science have already been exhausted. Millions of von Neumanns only accomplish logarithmically more work than one von Neumann, and it is not worth the cost of constructing such AIs. AI does not economically substitute for most cognitively skilled human labor, since even when smarter AIs can be built, humans can be produced more cheaply. Attempts are made to improve human intelligence via genetic engineering, or neuropharmaceuticals, or braincomputer interfaces, or cloning Einstein, etc.; but these attempts are foiled by the discovery that most “intelligence” is either unreproducible or not worth the cost of reproducing it. Moore’s Law breaks down decisively, not just because of increasing technological difficulties of miniaturization, but because ever-faster computer chips don’t accomplish much more than the previous generation of chips, and so there is insufficient economic incentive for Intel to build new factories. Life continues mostly as before, for however many more centuries. k ≈ 1 +, the “intelligence combustion”: Argument: Over the last many decades, world economic growth has been roughly exponential—growth has neither collapsed below exponential nor exploded above, implying a metaphorical k roughly equal to one (and slightly on the positive side). This is the characteristic behavior of a world full of smart cognitive agents making new scientific discoveries, inventing new technologies, and reinvesting resources to obtain further resources. There is no reason to suppose that changing from carbon to silicon will yield anything different. Furthermore, any single AI agent is unlikely to be significant compared to an economy of seven-plus billion humans. Thus AI progress will be dominated for some time by the contributions of the world economy to AI research, rather than by any one AI’s internal self-improvement. No one agent is capable of contributing more than a tiny fraction of the total progress in computer science, and this doesn’t change when human-equivalent AIs are invented.5 Scenario: The effect of introducing AIs to the global economy is a gradual, continuous increase in the overall rate of economic growth, since the first and most expensive AIs carry out a small part of the global economy’s cognitive labor. Over time, the cognitive labor of AIs becomes cheaper and constitutes a larger portion of the total economy. The timescale of exponential growth starts out at the level of a human-only economy and gradually, continuously shifts to a higher growth rate— for example, Hanson (1998b) predicts world economic doubling times of between a month and a year. Economic dislocations are unprecedented but take place on a timescale which gives humans some chance to react. Prompt k >> 1, the “intelligence explosion”: Argument: The history of hominid evolution to date shows that it has not required exponentially greater amounts of evolutionary optimization to produce substantial real-world gains in cognitive performance—it did not require ten times the evolutionary interval to go from Homo erectus to Homo sapiens as from Australopithecus to Homo erectus. 6 All compound interest returned on discoveries such as the invention of agriculture, or the invention of science, or the invention of computers, has occurred without any ability of humans to reinvest technological dividends to increase their brain sizes, speed up their neurons, or improve the low-level algorithms used by their neural circuitry. Since an AI can reinvest the fruits of its intelligence in larger brains, faster processing speeds, and improved low-level algorithms, we should expect an AI’s growth curves to be sharply above human growth curves. Scenario: The first machine intelligence system to achieve sustainable returns on cognitive reinvestment is able to vastly improve its intelligence relatively quickly—for example, by rewriting its own software or by buying (or stealing) access to orders of magnitude more hardware on clustered servers. Such an AI is “prompt critical”— it can reinvest the fruits of its cognitive investments on short timescales, without the need to build new chip factories first. By the time such immediately accessible improvements run out, the AI is smart enough to, for example, crack the problem of protein structure prediction. The AI emails DNA sequences to online peptide synthesis labs (some of which boast a seventy-two-hour turnaround time), and uses the resulting custom proteins to construct more advanced ribosome equivalents (molecular factories). Shortly afterward, the AI has its own molecular nanotechnology and can begin construction of much faster processors and other rapidly deployed, technologically advanced infrastructure. This rough sort of scenario is sometimes colloquially termed “hard takeoff ” or “AI-go-FOOM.”7 There are many questions we could proceed to ask about these stances, which are actually points along a spectrum that compresses several different dimensions of potentially independent variance, etc. The implications from the arguments to the scenarios are also disputable. Further sections will address some of this in greater detail. The broader idea is that different positions on “How large are the returns on cognitive reinvestment?” have widely different consequences with significant policy implications. The problem of investing resources to gain more resources is fundamental in economics. An (approximately) rational agency will consider multiple avenues for improvement, purchase resources where they are cheapest, invest where the highest returns are expected, and try to bypass any difficulties that its preferences do not explicitly forbid bypassing. This is one factor that makes an artificial intelligence unlike a heap of uranium bricks: if you insert a cadmium-foil rod into a heap of uranium bricks, the bricks will not try to shove the rod back out, nor reconfigure themselves so that the rod absorbs fewer valuable neutrons. In economics, it is routine to suggest that a rational agency will do its best to overcome, bypass, or intelligently reconfigure its activities around an obstacle. Depending on the AI’s preferences and capabilities, and on the surrounding society, it may make sense to steal poorly defended computing resources; returns on illegal investments are often analyzed in modern economic theory. Hence the problem of describing an AI’s curve for reinvested growth seems more like existing economics than existing problems in physics or computer science. As “microeconomics” is the discipline that considers rational agencies (such as individuals, firms, machine intelligences, and well-coordinated populations of machine intelligences) trying to maximize their returns on investment,8 the posed open problem about growth curves under cognitive investment and reinvestment is titled “Intelligence Explosion Microeconomics.” Section 2 of this paper discusses the basic language for talking about the intelligence explosion and argues that we should pursue this project by looking for underlying microfoundations, not by pursuing analogies to allegedly similar historical events. Section 3 attempts to showcase some specific informal reasoning about returns on cognitive investments, displaying the sort of arguments that have arisen in the context of the author explaining his stance on the intelligence explosion. Section 4 proposes a tentative methodology for formalizing theories of the intelligence explosion—a project of describing possible microfoundations and explicitly stating their alleged relation to historical experience, such that some possibilities can be falsified. Section 5 explores which subquestions seem both high value and possibly answerable. There are many things we’d like to know that we probably can’t know given a reasonable state of uncertainty about the domain—for example, when will an intelligence explosion occur? Section 6 summarizes and poses the open problem, and discusses what would be required for MIRI to fund further work in this area.

On (Extensionally) Defining Terms: It is obvious to ask questions like “What do you mean by ‘intelligence’?” or “What sort of AI system counts as ‘cognitively reinvesting’?” I shall attempt to answer these questions, but any definitions I have to offer should be taken as part of my own personal theory of the intelligence explosion. Consider the metaphorical position of early scientists who have just posed the question “Why is fire hot?” Someone then proceeds to ask, “What exactly do you mean by ‘fire’?” Answering, “Fire is the release of phlogiston” is presumptuous, and it is wiser to reply, “Well, for purposes of asking the question, fire is that bright orangey-red hot stuff coming out of that heap of sticks—which I think is really the release of phlogiston—but that definition is part of my answer, not part of the question itself.” I think it wise to keep this form of pragmatism firmly in mind when we are trying to define “intelligence” for purposes of analyzing the intelligence explosion.9 So as not to evade the question entirely, I usually use a notion of “intelligence ≡ efficient cross-domain optimization,” constructed as follows: 1. Consider optimization power as the ability to steer the future into regions of possibility ranked high in a preference ordering. For instance, Deep Blue has the power to steer a chessboard’s future into a subspace of possibility which it labels as “winning,” despite attempts by Garry Kasparov to steer the future elsewhere. Natural selection can produce organisms much more able to replicate themselves than the “typical” organism that would be constructed by a randomized DNA string—evolution produces DNA strings that rank unusually high in fitness within the space of all DNA strings.10 2. Human cognition is distinct from bee cognition or beaver cognition in that human cognition is significantly more generally applicable across domains: bees build hives and beavers build dams, but a human engineer looks over both and then designs a dam with a honeycomb structure. This is also what separates Deep Blue, which only played chess, from humans, who can operate across many different domains and learn new fields. 3. Human engineering is distinct from natural selection, which is also a powerful cross-domain consequentialist optimizer, in that human engineering is faster and more computationally efficient. (For example, because humans can abstract over the search space, but that is a hypothesis about human intelligence, not part of my definition.) In combination, these yield a definition of “intelligence ≡ efficient cross-domain optimization.” This tries to characterize “improved cognition” as the ability to produce solutions higher in a preference ordering, including, for example, a chess game with a higher probability of winning than a randomized chess game, an argument with a higher probability of persuading a human target, a transistor connection diagram that does more floatingpoint operations per second than a previous CPU, or a DNA string corresponding to a protein unusually apt for building a molecular factory. Optimization is characterized by an ability to hit narrow targets in a search space, where demanding a higher ranking in a preference ordering automatically narrows the measure of equally or more preferred outcomes. Improved intelligence is then hitting a narrower target in a search space, more computationally efficiently, via strategies that operate across a wider range of domains. That definition is one which I invented for other purposes (my work on machine intelligence as such) and might not be apt for reasoning about the intelligence explosion. For purposes of discussing the intelligence explosion, it may be wiser to reason about forms of growth that more directly relate to quantities we can observe. The narrowness of the good-possibility space attained by a search process does not correspond very directly to most historical observables. And for purposes of posing the question of the intelligence explosion, we may be better off with “Intelligence is that sort of smartish stuff coming out of brains, which can play chess, and price bonds, and persuade people to buy bonds, and invent guns, and figure out gravity by looking at wandering lights in the sky; and which, if a machine intelligence had it in large quantities, might let it invent molecular nanotechnology; and so on.” To frame it another way, if something is powerful enough to build a Dyson Sphere, it doesn’t really matter very much whether we call it “intelligent” or not. And this is just the sort of “intelligence” we’re interested in—something powerful enough that whether or not we define it as “intelligent” is moot. This isn’t to say that definitions are forbidden—just that further definitions would stake the further claim that those particular definitions were apt for carving reality at its joints, with respect to accurately predicting an intelligence explosion. Choice of definitions has no power to affect physical reality. If you manage to define “AI self-improvement” in such a way as to exclude some smartish computer-thingy which carries out some mysterious internal activities on its own code for a week and then emerges with a solution to protein structure prediction which it uses to build its own molecular nanotechnology . . . then you’ve obviously picked the wrong definition of “self-improvement.” See, for example, the definition advocated by Mahoney (2010) in which “self-improvement” requires an increase in Kolmogorov complexity of an isolated system, or Bringsjord’s (2012) definition in which a Turing machine is only said to selfimprove if it can raise itself into a class of hypercomputers. These are both definitions which strike me as inapt for reasoning about the intelligence explosion, since it is not obvious (in fact I think it obviously false) that this sort of “self-improvement” is required to invent powerful technologies. One can define self-improvement to be the increase in Kolmogorov complexity of an isolated deterministic system, and proceed to prove that this can only go as the logarithm of time. But all the burden of showing that a realworld intelligence explosion is therefore impossible rests on the argument that doing impactful things in the real world requires an isolated machine intelligence to increase its Kolmogorov complexity. We should not fail to note that this is blatantly false.11 This doesn’t mean that we should never propose more sophisticated definitions of selfimprovement. It means we shouldn’t lose sight of the wordless pragmatic background concept of an AI or AI population that rewrites its own code, or writes a successor version of itself, or writes an entirely new AI, or builds a better chip factory, or earns money to purchase more server time, or otherwise does something that increases the amount of pragmatically considered cognitive problem-solving capability sloshing around the system. And beyond that, “self-improvement” could describe genetically engineered humans, or humans with brain-computer interfaces, or upload clades, or several other possible scenarios of cognitive reinvestment, albeit here I will focus on the case of machine intelligence.12 It is in this spirit that I pose the open problem of formalizing I. J. Good’s notion of the intelligence explosion. Coming up with good definitions for informal terms like “cognitive reinvestment,” as they appear in the posed question, can be considered as part of the problem. In further discussion I suggest various definitions, categories, and distinctions. But such suggestions are legitimately disputable by anyone who thinks that a different set of definitions would be better suited to carving reality at its joints—to predicting what we will, in reality, actually observe to happen once some sort of smartish agency tries to invest in becoming smarterish.

Issues to Factor Out: Although we are ultimately interested only in the real-world results, I suggest that it will be productive theoretically—carve the issues at their natural joints—if we factor out for separate consideration issues of whether, for example, there might be an effective monitoring regime which could prevent an intelligence explosion, or whether the entire world economy will collapse due to global warming before then, and numerous other issues that don’t seem to interact very strongly with the returns on cognitive investment qua cognitive investment. 13 In particular, I would suggest explicitly factoring out all considerations of “What if an agent’s preferences are such that it does not want to increase capability at the fastest rate it can achieve?” As Omohundro (2008) and Bostrom (2012) point out, most possible preferences imply capability increase as an instrumental motive. If you want to build an intergalactic civilization full of sentient beings leading well-lived lives, you will want access to energy and matter. The same also holds true if you want to fill space with two-hundred-meter giant cheesecakes. In either case you will also have an instrumental goal of becoming smarter. Just as you can fulfill most goals better by having access to more material resources, you can also accomplish more by being better at cognitive problems—by being able to hit narrower targets in a search space. The space of all possible mind designs is vast (Muehlhauser and Salamon 2012), and there will always be some special case of an agent that chooses not to carry out any given deed (Armstrong, forthcoming). Given sufficient design competence, it should thus be possible to design an agent that doesn’t prefer to ascend at the maximum possible rate— though expressing this within the AI’s own preferences I would expect to be structurally nontrivial. Even so, we need to separately consider the question of how fast a rational agency could intelligence-explode if it were trying to self-improve as fast as possible. If the maximum rate of ascent is already inherently slow, then there is little point in constructing a special AI design that prefers not to improve faster than its programmers can verify. Policies are motivated by differentials of expected utility; there’s no incentive to do any sort of action X intended to prevent Y unless we predict that Y might otherwise tend to follow assuming not-X. This requires us to set aside the proposed slowing factor and talk about what a rational agency might do if not slowed. Thus I suggest that initial investigations of the intelligence explosion should consider the achievable rate of return on cognitive reinvestment for a rational agency trying to self-improve as fast as possible, in the absence of any obstacles not already present in today’s world.14 This also reflects the hope that trying to tackle the posed Open Problem should not require expertise in Friendly AI or international politics in order to talk about the returns on cognitive investment qua investment, even if predicting actual real-world outcomes might (or might not) require some of these issues to be factored back in. 

AI Preferences: A Brief Summary of Core Theses: Despite the above, it seems impossible not to at least briefly summarize some of the state of discussion on AI preferences—if someone believes that a sufficiently powerful AI, or one which is growing at a sufficiently higher rate than the rest of humanity and hence gaining unsurpassable advantages, is unavoidably bound to kill everyone, then they may have a hard time dispassionately considering and analyzing the potential growth curves. I have suggested that, in principle and in difficult practice, it should be possible to design a “Friendly AI” with programmer choice of the AI’s preferences, and have the AI self-improve with sufficiently high fidelity to knowably keep these preferences stable. I also think it should be possible, in principle and in difficult practice, to convey the complicated information inherent in human preferences into an AI, and then apply further idealizations such as reflective equilibrium and ideal advisor theories (Muehlhauser and Williamson 2013) so as to arrive at an output which corresponds intuitively to the AI “doing the right thing.” See also Yudkowsky (2008a). On a larger scale the current state of discussion around these issues seems to revolve around four major theses: The Intelligence Explosion Thesis says that, due to recursive self-improvement, an AI can potentially grow in capability on a timescale that seems fast relative to human experience. This in turn implies that strategies which rely on humans reacting to and restraining or punishing AIs are unlikely to be successful in the long run, and that what the first strongly self-improving AI prefers can end up mostly determining the final outcomes for Earth-originating intelligent life. (This subthesis is the entire topic of the current paper. One observes that the arguments surrounding the thesis are much more complex than the simple summary above would suggest. This is also true of the other three theses below.) The Orthogonality Thesis says that mind-design space is vast enough to contain minds with almost any sort of preferences. There exist instrumentally rational agents which pursue almost any utility function, and they are mostly stable under reflection. See Armstrong (forthcoming) and Muehlhauser and Salamon (2012). There are many strong arguments for the Orthogonality Thesis, but one of the strongest proceeds by construction: If it is possible to answer the purely epistemic question of which actions would lead to how many paperclips existing, then a paperclip-seeking agent is constructed by hooking up that answer to motor output. If it is very good at answering the epistemic question of which actions would result in great numbers of paperclips, then it will be a very instrumentally powerful agent.15 The Complexity of Value Thesis says that human values are complex in the sense of having high algorithmic (Kolmogorov) complexity (Yudkowsky 2011; Muehlhauser and Helm 2012). Even idealized forms of human value, such as reflective equilibrium (Rawls 1971) or ideal advisor theories (Rosati 1995)—what we would want in the limit of infinite knowledge of the world, infinite thinking speeds, and perfect self-understanding, etc.—are predicted to still have high algorithmic complexity. This tends to follow from naturalistic theories of metaethics under which human preferences for happiness, freedom, growth, aesthetics, justice, etc., (see Frankena [1973, chap. 5] for one list of commonly stated terminal values) have no privileged reason to be readily reducible to each other or to anything else. The Complexity of Value Thesis is that to realize valuable outcomes, an AI must have complex information in its utility function; it also will not suffice to tell it to “just make humans happy” or any other simplified, compressed principle.16 The Instrumental Convergence Thesis says that for most choices of a utility function, instrumentally rational agencies will predictably wish to obtain certain generic resources, such as matter and energy, and pursue certain generic strategies, such as not making code changes which alter their effective future preferences (Omohundro 2008; Bostrom 2012). Instrumental Convergence implies that an AI does not need to have specific terminal values calling for it to harm humans, in order for humans to be harmed. The AI does not hate you, but neither does it love you, and you are made of atoms that it can use for something else. In combination, the Intelligence Explosion Thesis, the Orthogonality Thesis, the Complexity of Value Thesis, and the Instrumental Convergence Thesis imply a very large utility differential for whether or not we can solve the design problems (1) relating to a self-improving AI with stable specifiable preferences and (2) relating to the successful transfer of human values (and their further idealization via, e.g., reflective equilibrium or ideal advisor theories), with respect to the first AI to undergo the intelligence explosion. All this is another and quite different topic within the larger discussion of the intelligence explosion, compared to its microeconomics. Here I will only note that large returns on cognitive investment need not correspond to unavoidable horror scenarios so painful that we are forced to argue against them, nor to virtuous pro-science-andtechnology scenarios that virtuous people ought to affiliate with. For myself I would tend to view larger returns on cognitive reinvestment as corresponding to increased policydependent variance. And whatever the true values of the unseen variables, the question is not whether they sound like “good news” or “bad news”; the question is how we can improve outcomes as much as possible given those background settings. 

Microfoundations of Growth: Consider the stance on the intelligence explosion thesis which says: “I think we should expect that exponentially greater investments—of computing hardware, software programming effort, etc.—will only produce linear gains in real-world performance on cognitive tasks, since most search spaces are exponentially large. So the fruits of machine intelligence reinvested into AI will only get logarithmic returns on each step, and the ‘intelligence explosion’ will peter out very quickly.” Is this scenario plausible or implausible? Have we seen anything in the real world— made any observation, ever—that should affect our estimate of its probability? (At this point, I would suggest that the serious reader turn away and take a moment to consider this question on their own before proceeding.) Some possibly relevant facts might be: • Investing exponentially more computing power into a constant chess-playing program produces linear increases in the depth of the chess-game tree that can be searched, which in turn seems to correspond to linear increases in Elo rating (where two opponents of a fixed relative Elo distance, regardless of absolute ratings, theoretically have a constant probability of losing or winning to each other). • Chess-playing algorithms have recently improved much faster than chess-playing hardware, particularly since chess-playing programs began to be open-sourced. Deep Blue ran on 11.8 billion floating-point operations per second and had an Elo rating of around 2,700; Deep Rybka 3 on a Intel Core 2 Quad 6600 has an Elo rating of 3,202 on 2.4 billion floating-point operations per second.17 • It seems that in many important senses, humans get more than four times the real-world return on our intelligence compared to our chimpanzee cousins. This was achieved with Homo sapiens having roughly four times as much cortical volume and six times as much prefrontal cortex.18 • Within the current human species, measured IQ is entangled with brain size; and this entanglement is around a 0.3 correlation in the variances, rather than, say, a doubling of brain size being required for each ten-point IQ increase.19 • The various Moore’s-like laws measuring computing technologies, operations per second, operations per dollar, disk space per dollar, and so on, are often said to have characteristic doubling times ranging from twelve months to three years; they are formulated so as to be exponential with respect to time. People have written papers questioning Moore’s Law’s validity (see, e.g., Tuomi [2002]); and the Moore’s-like law for serial processor speeds broke down in 2004. The original law first observed by Gordon Moore, over transistors per square centimeter, has remained on track. • Intel has invested exponentially more researcher-hours and inflation-adjusted money to invent the technology and build the manufacturing plants for successive generations of CPUs. But the CPUs themselves are increasing exponentially in transistor operations per second, not linearly; and the computer-power doubling time is shorter (that is, the exponent is higher) than that of the increasing investment cost.20 • The amount of evolutionary time (a proxy measure of cumulative selection pressure and evolutionary optimization) which produced noteworthy changes during human and hominid evolution does not seem to reveal exponentially greater amounts of time invested. It did not require ten times as long to go from Homo erectus to Homo sapiens, as from Australopithecus to Homo erectus. 21 • World economic output is roughly exponential and increases faster than population growth, which is roughly consistent with exponentially increasing investments producing exponentially increasing returns. That is, roughly linear (but with multiplication factor k > 1) returns on investment. On a larger timescale, world-historical economic output can be characterized as a sequence of exponential modes (Hanson 1998b). Total human economic output was also growing exponentially in AD 1600 or 2000 BC, but with smaller exponents and much longer doubling times. • Scientific output in “total papers written” tends to grow exponentially with a short doubling time, both globally (around twenty-seven years [NSB 2012, chap. 5]) and within any given field. But it seems extremely questionable whether there has been more global change from 1970 to 2010 than from 1930 to 1970. (For readers who have heard relatively more about “accelerating change” than about “the Great Stagnation”: the claim is that total-factor productivity growth in, e.g., the United States dropped from 0.75% per annum before the 1970s to 0.25% thereafter [Cowen 2011].) A true cynic might claim that, in many fields, exponentially greater investment in science is yielding a roughly constant amount of annual progress— sublogarithmic returns!22 • This graph (Silver 2012) shows how many books were authored in Europe as a function of time; after the invention of the printing press, the graph jumps in a sharp, faster-than-exponential upward surge. • All technological progress in known history has been carried out by essentially constant human brain architectures. There are theses about continuing human evolution over the past ten thousand years, but all such changes are nowhere near the scale of altering “You have a brain that’s more or less 1,250 cubic centimeters of dendrites and axons, wired into a prefrontal cortex, a visual cortex, a thalamus, and so on.” It has not required much larger brains, or much greater total cumulative selection pressures, to support the continuing production of more sophisticated technologies and sciences over the human regime. • The amount of complex order per unit time created by a human engineer is completely off the scale compared to the amount of complex order per unit time created by natural selection within a species. A single mutation conveying a 3% fitness advantage would be expected to take 768 generations to rise to fixation through a sexually reproducing population of a hundred thousand members. A computer programmer can design new complex mechanisms with hundreds of interoperating parts over the course of a day or an hour. In turn, the amount of complex order per unit time created by natural selection is completely off the scale for Earth before the dawn of life. A graph of “order created per unit time” during Earth’s history would contain two discontinuities representing the dawn of fundamentally different optimization processes. The list of observations above might give you the impression that it could go either way—that some things are exponential and some things aren’t. Worse, it might look like an invitation to decide your preferred beliefs about AI self-improvement as a matter of emotional appeal or fleeting intuition, and then decide that any of the above cases which behave similarly to how you think AI self-improvement should behave, are the natural historical examples we should consult to determine the outcome of AI. For example, clearly the advent of self-improving AI seems most similar to other economic speedups like the invention of agriculture.23 Or obviously it’s analogous to other foundational changes in the production of complex order, such as human intelligence or self-replicating life.24 Or self-evidently the whole foofaraw is analogous to the panic over the end of the Mayan calendar in 2012 since it belongs in the reference class of “supposed big future events that haven’t been observed.”25 For more on the problem of “reference class tennis,” see section 2.1. It seems to me that the real lesson to be derived from the length of the above list is that we shouldn’t expect some single grand law about whether you get superexponential, exponential, linear, logarithmic, or constant returns on cognitive investments. The cases above have different behaviors; they are not all conforming to a single Grand Growth Rule. It’s likewise not the case that Reality proceeded by randomly drawing a curve type from a barrel to assign to each of these scenarios, and the curve type of “AI self-improvement” will be independently sampled with replacement from the same barrel. So it likewise doesn’t seem valid to argue about how likely it is that someone’s personal favorite curve type gets drawn by trumpeting historical cases of that curve type, thereby proving that it’s more frequent within the Curve Type Barrel and more likely to be randomly drawn. Most of the processes cited above yielded fairly regular behavior over time. Meaning that the attached curve was actually characteristic of that process’s causal mechanics, and a predictable feature of those mechanics, rather than being assigned and reassigned at random. Anyone who throws up their hands and says, “It’s all unknowable!” may also be scoring fewer predictive points than they could. These differently behaving cases are not competing arguments about how a single grand curve of cognitive investment has previously operated. They are all simultaneously true, and hence they must be telling us different facts about growth curves—telling us about different domains of a multivariate growth function—advising us of many compatible truths about how intelligence and real-world power vary with different kinds of cognitive investments.26 Thus it’s not possible that the facts listed are all “strong” arguments, about the same variable, pointing in different directions. Rather than selecting one particular historical curve to anoint as characteristic of the intelligence explosion, it might be possible to build an underlying causal model, one which would be compatible with all these separate facts. I would propose that we should be trying to formulate a microfoundational model which, rather than just generalizing over surface regularities, tries to describe underlying causal processes and returns on particular types of cognitive investment. For example, rather than just talking about how chess programs have improved over time, we might try to describe how chess programs improve as a function of computing resources plus the cumulative time that human engineers spend tweaking the algorithms. Then in turn we might say that human engineers have some particular intelligence oroptimization power, which is different from the optimization power of a chimpanzee or the processes of natural selection. The process of building these causal models would hopefully let us arrive at a more realistic picture—one compatible with the many different growth curves observed in different historical situations. 

The Outside View versus the Lucas Critique: A fundamental tension in the so-far-informal debates on intelligence explosion has been the rough degree of abstraction that is trustworthy and useful when modeling these future events. The first time I happened to occupy the same physical room as Ray Kurzweil, I asked him why his graph of Moore’s Law showed the events of “a $1,000 computer is as powerful as a human brain,” “a $1,000 computer is a thousand times as powerful as a human brain,” and “a $1,000 computer is a billion times as powerful as a human brain,” all following the same historical trend of Moore’s Law.27 I asked, did it really make sense to continue extrapolating the humanly observed version of Moore’s Law past the point where there were putatively minds with a billion times as much computing power? Kurzweil2001 replied that the existence of machine superintelligence was exactly what would provide the fuel for Moore’s Law to continue and make it possible to keep developing the required technologies. In other words, Kurzweil2001 regarded Moore’s Law as the primary phenomenon and considered machine superintelligence a secondary phenomenon which ought to assume whatever shape was required to keep the primary phenomenon on track.28 You could even imagine arguing (though Kurzweil2001 did not say this part) that we’ve seen Moore’s Law continue through many generations and across many different types of hardware, while we have no actual experience with machine superintelligence. So an extrapolation of Moore’s Law should take epistemic primacy over more speculative predictions about superintelligence because it’s based on more experience and firmer observations. My own interpretation of the same history would be that there was some underlying difficulty curve for how more sophisticated CPUs required more knowledge and better manufacturing technology to build, and that over time human researchers exercised their intelligence to come up with inventions, tools to build more inventions, physical theories, experiments to test those theories, programs to help design CPUs,29 etc. The process whereby more and more transistors are packed into a given area every eighteen months should not be an exogenous factor of how often the Earth traverses 1.5 orbits around the Sun; it should be a function of the engineers. So if we had faster engineers, we would expect a faster form of Moore’s Law. (See section 3.3 for related points and counterpoints about fast manipulator technologies and sensor bandwidth also being required.) Kurzweil2001 gave an impromptu response seeming to suggest that Moore’s Law might become more difficult at the same rate that superintelligence increased in problemsolving ability, thus preserving the forecast for Moore’s Law in terms of time. But why should that be true? We don’t have an exact idea of what the historical intrinsic-difficulty curve looked like; it’s difficult to observe directly. Our main data is the much-betterknown Moore’s Law trajectory which describes how fast human engineers were able to traverse the difficulty curve over outside time.30 But we could still reasonably expect that, if our old extrapolation was for Moore’s Law to follow such-and-such curve given human engineers, then faster engineers should break upward from that extrapolation. Or to put it more plainly, the fully-as-naive extrapolation in the other direction would be, “Given human researchers of constant speed, computing speeds double every 18 months. So if the researchers are running on computers themselves, we should expect computing speeds to double in 18 months, then double again in 9 physical months (or 18 subjective months for the 2x-speed researchers), then double again in 4.5 physical months, and finally reach infinity after a total of 36 months.” If humans accumulate subjective time at a constant rate x = t, and we observe that computer speeds increase as a Moore’s-Law exponential function of subjective time y = e x , then when subjective time increases at the rate of current computer speeds we get the differential equation y 0 = e y whose solution has computer speeds increasing hyperbolically, going to infinity after finite time.31 (See, e.g., the model of Moravec [1999].) In real life, we might not believe this as a quantitative estimate. We might not believe that in real life such a curve would have, even roughly, a hyperbolic shape before it started hitting (high) physical bounds. But at the same time, we might in real life believe that research ought to go substantially faster if the researchers could reinvest the fruits of their labor into their own cognitive speeds—that we are seeing an important hint buried within this argument, even if its details are wrong. We could believe as a qualitative prediction that “if computer chips are following Moore’s Law right now with human researchers running at constant neural processing speeds, then in the hypothetical scenario where the researchers are running on computers, we should see a new Moore’s Law bounded far below by the previous one.” You might say something like, “Show me a reasonable model of how difficult it is to build chips as a function of knowledge, and how knowledge accumulates over subjective time, and you’ll get a hyperexponential explosion out of Moore’s Law once the researchers are running on computers. Conversely, if you give me a regular curve of increasing difficulty which averts an intelligence explosion, it will falsely retrodict that human engineers should only be able to get subexponential improvements out of computer technology. And of course it would be unreasonable—a specific unsupported miraculous irregularity of the curve—for making chips to suddenly get much more difficult to build, coincidentally exactly as AIs started doing research. The difficulty curve might shift upward at some random later point, but there’d still be a bonanza from whatever improvement was available up until then.” In turn, that reply gets us into a rather thorny meta-level issue: A: Why are you introducing all these strange new unobservable abstractions? We can see chips getting faster over time. That’s what we can measure and that’s what we have experience with. Who measures this difficulty of which you speak? Who measures knowledge? These are all made-up quantities with no rigorous basis in reality. What we do have solid observations of is the number of transistors on a computer chip, per year. So I’m going to project that extremely regular curve out into the future and extrapolate from there. The rest of this is sheer, loose speculation. Who knows how many other possible supposed “underlying” curves, besides this “knowledge” and “difficulty” business, would give entirely different answers? To which one might reply: B: Seriously? Let’s consider an extreme case. Neurons spike around 2–200 times per second, and axons and dendrites transmit neural signals at 1–100 meters per second, less than a millionth of the speed of light. Even the heat dissipated by each neural operation is around six orders of magnitude above the thermodynamic minimum at room temperature.32 Hence it should be physically possible to speed up “internal” thinking (which doesn’t require “waiting on the external world”) by at least six orders of magnitude without resorting to smaller, colder, reversible, or quantum computers. Suppose we were dealing with minds running a million times as fast as a human, at which rate they could do a year of internal thinking in thirty-one seconds, such that the total subjective time from the birth of Socrates to the death of Turing would pass in 20.9 hours. Do you still think the best estimate for how long it would take them to produce their next generation of computing hardware would be 1.5 orbits of the Earth around the Sun? Two well-known epistemological stances, with which the respective proponents of these positions could identify their arguments, would be the outside view and the Lucas critique. The “outside view” (Kahneman and Lovallo 1993) is a term from the heuristics and biases program in experimental psychology. A number of experiments show that if you ask subjects for estimates of, say, when they will complete their Christmas shopping, the right question to ask is, “When did you finish your Christmas shopping last year?” and not, “How long do you think it will take you to finish your Christmas shopping?” The latter estimates tend to be vastly over-optimistic, and the former rather more realistic. In fact, as subjects are asked to make their estimates using more detail—visualize where, when, and how they will do their Christmas shopping—their estimates become more optimistic, and less accurate. Similar results show that the actual planners and implementers of a project, who have full acquaintance with the internal details, are often much more optimistic and much less accurate in their estimates compared to experienced outsiders who have relevant experience of similar projects but don’t know internal details. This is sometimes called the dichotomy of the inside view versus the outside view. The “inside view” is the estimate that takes into account all the details, and the “outside view” is the very rough estimate that would be made by comparing your project to other roughly similar projects without considering any special reasons why this project might be different. The Lucas critique (Lucas 1976) in economics was written up in 1976 when “stagflation”—simultaneously high inflation and unemployment—was becoming a problem in the United States. Robert Lucas’s concrete point was that the Phillips curve trading off unemployment and inflation had been observed at a time when the Federal Reserve was trying to moderate inflation. When the Federal Reserve gave up on moderating inflation in order to drive down unemployment to an even lower level, employers and employees adjusted their long-term expectations to take into account continuing inflation, and the Phillips curve shifted. Lucas’s larger and meta-level point was that the previously observed Phillips curve wasn’t fundamental enough to be structurally invariant with respect to Federal Reserve policy—the concepts of inflation and unemployment weren’t deep enough to describe elementary things that would remain stable even as Federal Reserve policy shifted. A very succinct summary appears in Wikipedia (2013): The Lucas critique suggests that if we want to predict the effect of a policy experiment, we should model the “deep parameters” (relating to preferences, technology and resource constraints) that are assumed to govern individual behavior; so called “microfoundations.” If these models can account for observed empirical regularities, we can then predict what individuals will do, taking into account the change in policy, and then aggregate the individual decisions to calculate the macroeconomic effects of the policy change. The main explicit proponent of the outside view in the intelligence explosion debate is Robin Hanson, who also proposes that an appropriate reference class into which to place the “Singularity”—a term not specific to the intelligence explosion but sometimes including it—would be the reference class of major economic transitions resulting in substantially higher exponents of exponential growth. From Hanson’s (2008a) blog post “Outside View of Singularity”: Most everything written about a possible future singularity takes an inside view, imagining details of how it might happen. Yet people are seriously biased toward inside views, forgetting how quickly errors accumulate when reasoning about details. So how far can we get with an outside view of the next singularity? Taking a long historical long view, we see steady total growth rates punctuated by rare transitions when new faster growth modes appeared with little warning. We know of perhaps four such “singularities”: animal brains (∼600 MYA), humans (∼2 MYA), farming (∼10 KYA), and industry (∼0.2 KYA). The statistics of previous transitions suggest we are perhaps overdue for another one, and would be substantially overdue in a century. The next transition would change the growth rate rather than capabilities directly, would take a few years at most, and the new doubling time would be a week to a month. More on this analysis can be found in Hanson (1998b). The original blog post concludes: Excess inside viewing usually continues even after folks are warned that outside viewing works better; after all, inside viewing better shows off inside knowledge and abilities. People usually justify this via reasons why the current case is exceptional. (Remember how all the old rules didn’t apply to the new dotcom economy?) So expect to hear excuses why the next singularity is also an exception where outside view estimates are misleading. Let’s keep an open mind, but a wary open mind. Another of Hanson’s (2008c) posts, in what would later be known as the YudkowskyHanson AI-Foom Debate, said: It is easy, way too easy, to generate new mechanisms, accounts, theories, and abstractions. To see if such things are useful, we need to vet them, and that is easiest “nearby,” where we know a lot. When we want to deal with or understand things “far,” where we know little, we have little choice other than to rely on mechanisms, theories, and concepts that have worked well near. Far is just the wrong place to try new things. There are a bazillion possible abstractions we could apply to the world. For each abstraction, the question is not whether one can divide up the world that way, but whether it “carves nature at its joints,” giving useful insight not easily gained via other abstractions. We should be wary of inventing new abstractions just to make sense of things far; we should insist they first show their value nearby. The lesson of the outside view pushes us to use abstractions and curves that are clearly empirically measurable, and to beware inventing new abstractions that we can’t see directly. The lesson of the Lucas critique pushes us to look for abstractions deep enough to describe growth curves that would be stable in the face of minds improving in speed, size, and software quality. You can see how this plays out in the tension between “Let’s predict computer speeds using this very well-measured curve for Moore’s Law over time—where the heck is all this other stuff coming from?” versus “But almost any reasonable causal model that describes the role of human thinking and engineering in producing better computer chips, ought to predict that Moore’s Law would speed up once computer-based AIs were carrying out all the research!” It would be unfair to use my passing exchange with Kurzweil as a model of the debate between myself and Hanson. Still, I did feel that the basic disagreement came down to a similar tension—that Hanson kept raising a skeptical and unmoved eyebrow at the wildeyed, empirically unvalidated, complicated abstractions which, from my perspective, constituted my attempt to put any sort of microfoundations under surface curves that couldn’t possibly remain stable. Hanson’s overall prototype for visualizing the future was an economic society of ems, software emulations of scanned human brains. It would then be possible to turn capital inputs (computer hardware) into skilled labor (copied ems) almost immediately. This was Hanson’s explanation for how the em economy could follow the “same trend” as past economic speedups, to a world economy that doubled every year or month (vs. a roughly fifteen-year doubling time at present [Hanson 1998b]). I thought that the idea of copying human-equivalent minds missed almost every potentially interesting aspect of the intelligence explosion, such as faster brains, larger brains, or above all better-designed brains, all of which seemed liable to have far greater effects than increasing the quantity of workers. Why? That is, if you can invest a given amount of computing power in more brains, faster brains, larger brains, or improving brain algorithms, why think that the return on investment would be significantly higher in one of the latter three cases? A more detailed reply is given in section 3, but in quick summary: There’s a saying in software development, “Nine women can’t have a baby in one month,” meaning that you can’t get the output of ten people working for ten years by hiring a hundred people to work for one year, or more generally, that working time scales better than the number of people, ceteris paribus. It’s also a general truth of computer science that fast processors can simulate parallel processors but not always the other way around. Thus we’d expect the returns on speed to be higher than the returns on quantity. We have little solid data on how human intelligence scales with added neurons and constant software. Brain size does vary between humans and this variance correlates by about 0.3 with g (McDaniel 2005), but there are reams of probable confounders, such as childhood nutrition. Humans have around four times the brain volume of chimpanzees, but the difference between us is probably mostly brain-level cognitive algorithms.33 It is a general truth of computer science that if you take one processing unit and split it up into ten parts with limited intercommunication bandwidth, they can do no better than the original on any problem, and will do considerably worse on many problems. Similarly we might expect that, for most intellectual problems, putting on ten times as many researchers running human software scaled down to one-fifth the brain size would probably not be a net gain, and that, for many intellectual problems, researchers with four times the brain size would probably be a significantly greater gain than adding four times as many researchers.34 Trying to say how intelligence and problem-solving ability scale with improved cognitive algorithms is even harder to relate to observation. In any computer-based field where surface capabilities are visibly improving, it is usually true that you are better off with modern algorithms and a computer from ten years earlier, compared to a modern computer and the algorithms from ten years earlier. This is definitely true in computer chess, even though the net efforts put in by chess-program enthusiasts to create better programs are small compared to the vast effort Intel puts into creating better computer chips every year. But this observation only conveys a small fraction of the idea that you can’t match a human’s intellectual output using any number of chimpanzees. Informally, it looks to me like quantity < (size,speed) < quality when it comes to minds. Hanson’s scenario in which all investments went into increasing the mere quantity of ems—and this was a good estimate of the total impact of an intelligence explosion— seemed to imply that the returns on investment from larger brains, faster thinking, and improved brain designs could all be neglected, which implied that the returns from such investments were relatively low.35 Whereas it seemed to me that any reasonable microfoundations which were compatible with prior observation—which didn’t retrodict that a human should be intellectually replaceable by ten chimpanzees—should imply that quantity of labor wouldn’t be the dominating factor. Nonfalsified growth curves ought to say that, given an amount of computing power which you could invest in more minds, faster minds, larger minds, or better-designed minds, you would invest in one of the latter three. We don’t invest in larger human brains because that’s impossible with current technology— we can’t just hire a researcher with three times the cranial volume, we can only throw more warm bodies at the problem. If that investment avenue suddenly became available . . . it would probably make quite a large difference, pragmatically speaking. I was happy to concede that my model only made vague qualitative predictions—I didn’t think I had enough data to make quantitative predictions like Hanson’s estimates of future economic doubling times. But qualitatively I thought it obvious that all these hard-to-estimate contributions from faster brains, larger brains, and improved underlying cognitive algorithms were all pointing along the same rough vector, namely “way up.” Meaning that Hanson’s estimates, sticking to extrapolated curves of well-observed quantities, would be predictably biased way down. Whereas from Hanson’s perspective, this was all wild-eyed unverified speculation, and he was sticking to analyzing ems because we had a great deal of data about how human minds worked and no way to solidly ground all these new abstractions I was hypothesizing. Aside from the Lucas critique, the other major problem I have with the “outside view” is that everyone who uses it seems to come up with a different reference class and a different answer. To Ray Kurzweil, the obvious reference class for “the Singularity” is Moore’s Law as it has operated over recent history, not Hanson’s comparison to agriculture. In this post an online discussant of these topics places the “Singularity” into the reference class “beliefs in coming of a new world” which has “a 0% success rate” . . . explicitly terming this the proper “outside view” of the situation using “reference class forecasting,” and castigating anyone who tried to give a different answer as having used an “inside view.” For my response to all this at greater length, see “‘Outside View!’ as Conversation-Halter” (Yudkowsky 2010). The gist of my reply was that the outside view has been experimentally demonstrated to beat the inside view for software projects that are similar to previous software projects, and for this year’s Christmas shopping, which is highly similar to last year’s Christmas shopping. The outside view would be expected to work less well on a new thing that is less similar to the old things than all the old things were similar to each other—especially when you try to extrapolate from one kind of causal system to a very different causal system. And one major sign of trying to extrapolate across too large a gap is when everyone comes up with a different “obvious” reference class. Of course it also often happens that disputants think different microfoundations— different causal models of reality—are “obviously” appropriate. But then I have some idea of how to zoom in on hypothesized causes, assess their simplicity and regularity, and figure out how to check them against available evidence. I don’t know what to do after two people take different reference classes and come up with different outside views both of which we ought to just accept. My experience is that people end up doing the equivalent of saying, “I’m taking my reference class and going home.” A final problem I have with many cases of “reference class forecasting” is that—in addition to everyone coming up with a different reference class—their final answers often seem more specific than I think our state of knowledge should allow. I don’t think you should be able to tell me that the next major growth mode will have a doubling time of between a month and a year. The alleged outside viewer claims to know too much, once they stake their all on a single preferred reference class. But then what I have just said is an argument for enforced humility—“I don’t know, so you can’t know either!”—and is automatically suspect on those grounds. It must be fully conceded and advised that complicated models are hard to fit to limited data, and that when postulating curves which are hard to observe directly or nail down with precision, there is a great deal of room for things to go wrong. It does not follow that “reference class forecasting” is a good solution, or even the merely best solution.